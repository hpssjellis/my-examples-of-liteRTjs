<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0" />
    <title>LiteRT.js Multi-Model Depth Estimation Demo</title>
    
    <script type="module">
        import * as LiteRT from 'https://cdn.jsdelivr.net/npm/@litertjs/core@0.2.1/+esm';
        import * as LiteRTInterop from 'https://cdn.jsdelivr.net/npm/@litertjs/tfjs-interop/+esm';
        import 'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js';
        import 'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgpu/dist/tf-backend-webgpu.js';
        
        // --- Model Configuration Data ---
        // New array of possible models for the select box
        const myDepthModels = [
            { 
                myName: "Depth-Anything V2 (Large)", 
                myUrl: 'https://huggingface.co/qualcomm/Depth-Anything-V2/resolve/main/Depth-Anything-V2_float.tflite',
                myInputShape: 518 // Expected input size
            },
            { 
                myName: "Depth-Anything V2 (Small)", 
                myUrl: 'https://huggingface.co/qualcomm/Depth-Anything-V2-Small/resolve/main/Depth-Anything-V2-Small_float.tflite',
                myInputShape: 518 
            },
            { 
                myName: "MobileNet-Depth", 
                myUrl: 'https://huggingface.co/qualcomm/MobileNet-Depth-Estimation/resolve/main/MobileNet-Depth-Estimation_float.tflite',
                myInputShape: 256
            }
        ];

        // --- Global Variables (my-prefixed) ---
        let myModel = undefined;
        let myIsPredicting = false;
        let myTimerIntervalId = null; 
        let myInitialPredictionStartTime = 0; 
        let myCurrentInputShape = 518; // Stores the input shape for the currently loaded model

        // --- DOM Element References ---
        const myVideoElement = document.getElementById('webcam');
        const myOverlayCanvas = document.getElementById('overlay');
        const myLoadButton = document.getElementById('loadModelButton');
        const myCameraSelect = document.getElementById('cameraSelect');
        const myModelSelect = document.getElementById('modelSelect'); // NEW element reference
        const myLoadStatusSpan = document.getElementById('myLoadStatus');
        const myAnalysisStatusSpan = document.getElementById('myAnalysisStatus');

        // --- Timer Functions ---

        /**
         * Starts a count-up timer in the load status span.
         */
        function myStartLoadTimer() {
            let myStartTime = performance.now();
            myLoadStatusSpan.textContent = '0.0s';

            myTimerIntervalId = setInterval(() => {
                let myElapsedTime = performance.now() - myStartTime;
                myLoadStatusSpan.textContent = `${(myElapsedTime / 1000).toFixed(1)}s`;
            }, 100);
        }

        /**
         * Stops and clears the count-up timer for loading.
         */
        function myStopLoadTimer(myFinalMessage) {
            if (myTimerIntervalId !== null) {
                clearInterval(myTimerIntervalId);
                myTimerIntervalId = null;
            }
            myLoadStatusSpan.textContent = myFinalMessage;
        }
        
        // --- Utility Functions ---

        /**
         * Safely stops the prediction loop and disposes of the current model.
         */
        function myStopPredictionAndReset() {
            if (myIsPredicting) {
                myIsPredicting = false;
                // Note: We don't dispose of the model immediately here 
                // because the animation loop might be in the middle of a promise chain.
                // We let the garbage collector handle the old model once the new one is loaded.
            }
            if (myModel) {
                // If a model is loaded, attempt to dispose of it
                // and clear the reference for the next load.
                // LiteRT models are LiteRT.LiteRtModel instances.
                try {
                    myModel.dispose();
                } catch(e) {
                    console.warn("Could not dispose of previous model gracefully:", e);
                }
                myModel = undefined;
            }
            myLoadButton.textContent = 'Load Model & Start';
            myLoadButton.disabled = false;
            myCameraSelect.disabled = false;
            myModelSelect.disabled = false;
            myStopLoadTimer('Ready');
            myAnalysisStatusSpan.textContent = 'N/A';
            myInitialPredictionStartTime = 0;
        }

        /**
         * Populates the model selection dropdown with pre-defined models.
         */
        function myPopulateModelSelect() {
            myModelSelect.innerHTML = ''; // Clear existing options
            myDepthModels.forEach((myModel, myIndex) => {
                const myOption = document.createElement('option');
                myOption.value = myModel.myUrl;
                myOption.text = myModel.myName;
                myOption.dataset.myInputShape = myModel.myInputShape; // Store input shape as a data attribute
                if (myIndex === 0) { // Select the first model by default
                    myOption.selected = true;
                }
                myModelSelect.appendChild(myOption);
            });
        }


        // --- Core Functions ---

        /**
         * Populates the camera selection dropdown with available video input devices.
         */
        async function myGetCameras() {
            if (!navigator.mediaDevices || !navigator.mediaDevices.enumerateDevices) {
                console.warn('enumerateDevices() not supported.');
                return;
            }

            try {
                // Request media access first to get device labels
                // Temporarily request stream to force permission request
                const myTempStream = await navigator.mediaDevices.getUserMedia({ video: true, audio: false });
                myTempStream.getTracks().forEach(track => track.stop());

                const myDevices = await navigator.mediaDevices.enumerateDevices();
                const myVideoDevices = myDevices.filter(myDevice => myDevice.kind === 'videoinput');
                
                myCameraSelect.innerHTML = '';
                
                if (myVideoDevices.length === 0) {
                    myCameraSelect.innerHTML = '<option>No Camera Found</option>';
                    myCameraSelect.disabled = true;
                    return;
                }

                myVideoDevices.forEach((myDevice, myIndex) => {
                    const myOption = document.createElement('option');
                    myOption.value = myDevice.deviceId;
                    myOption.text = myDevice.label || `Camera ${myIndex + 1}`; 
                    myCameraSelect.appendChild(myOption);
                });

                myCameraSelect.disabled = false;
                
            } catch (e) {
                console.error('Error listing devices or getting stream:', e);
                myCameraSelect.innerHTML = '<option>Access Denied or Error</option>';
            }
        }
        
        /**
         * Enables the webcam using the currently selected device ID.
         */
        async function myEnableWebcam(myDeviceId) {
            if (myVideoElement.srcObject) {
                myVideoElement.srcObject.getTracks().forEach(track => track.stop());
                myVideoElement.srcObject = null;
            }

            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                console.error('Browser API navigator.mediaDevices.getUserMedia not available');
                return;
            }
            
            const myVideoConfig = {
                'audio': false,
                'video': {
                    deviceId: myDeviceId ? { exact: myDeviceId } : undefined,
                    width: 640,
                    height: 480
                }
            };
            
            try {
                const myStream = await navigator.mediaDevices.getUserMedia(myVideoConfig);
                myVideoElement.srcObject = myStream;
                await new Promise(resolve => myVideoElement.onloadedmetadata = resolve);
                myVideoElement.play(); // Ensure video starts playing
                console.log('Video stream started successfully.');
            } catch (e) {
                console.error('Error enabling webcam:', e);
            }
        }

        /**
         * Loads the LiteRT environment and the Depth Estimation model, timing the steps.
         */
        async function myLoadModel() {
            // Check if a model is already running or being loaded
            if (myIsPredicting || myLoadButton.disabled) return; 

            // Safely stop any previous prediction before starting a new one
            myStopPredictionAndReset();

            const mySelectedOption = myModelSelect.options[myModelSelect.selectedIndex];
            const myModelUrl = mySelectedOption.value;
            // Get the required input shape from the data attribute
            myCurrentInputShape = parseInt(mySelectedOption.dataset.myInputShape, 10);

            myLoadButton.textContent = 'Loading...';
            myLoadButton.disabled = true;
            myCameraSelect.disabled = true;
            myModelSelect.disabled = true;
            myAnalysisStatusSpan.textContent = 'Waiting for Model...';

            try {
                // 1. Start Webcam with selected device
                await myEnableWebcam(myCameraSelect.value);

                // 2. Start the LOAD timer
                myStartLoadTimer();

                // 3. Load Model & Backend
                console.log('Setting up WebGPU backend...');
                await tf.setBackend('webgpu');
            
                await LiteRT.loadLiteRt('https://cdn.jsdelivr.net/npm/@litertjs/core@0.2.1/wasm/');
            
                const myTfBackend = tf.backend();
                LiteRT.setWebGpuDevice(myTfBackend.device);
                
                console.log('Loading and compiling model...');
                myModel = await LiteRT.loadAndCompile(myModelUrl, {
                    accelerator: 'webgpu',
                });

                // 4. Stop the LOAD timer and update status
                myStopLoadTimer(`Loaded: ${myLoadStatusSpan.textContent}`);
                
                // 5. Start Prediction Loop and time the initial compilation delay
                console.log('Model loaded. Starting prediction timer.');
                myLoadButton.textContent = 'Compiling...';
                myAnalysisStatusSpan.textContent = `Compiling (${myCurrentInputShape}x${myCurrentInputShape})...`;
                myInitialPredictionStartTime = performance.now(); // Start timer here

                myIsPredicting = true;
                myPredict();
                
            } catch (e) {
                myStopLoadTimer('Error');
                console.error('Error during model loading or setup:', e);
                myLoadButton.textContent = 'Error Loading Model';
            }
        }
        
        /**
         * Main prediction loop using requestAnimationFrame.
         */
        async function myPredict() {
            if (myModel === undefined || !myIsPredicting) {
                // If prediction was stopped mid-loop, we just return.
                return;
            }

            // --- Check if this is the first prediction run for timing ---
            if (myInitialPredictionStartTime > 0) {
                // This code runs *after* the heavy WebGPU compilation finishes
                await tf.backend().queue.onSubmittedWorkDone();
                
                const myInitialDelay = performance.now() - myInitialPredictionStartTime;
                myAnalysisStatusSpan.textContent = `Delay: ${(myInitialDelay / 1000).toFixed(2)}s`;
                myLoadButton.textContent = 'Running';
                myModelSelect.disabled = false; // Re-enable model select after first run
                myInitialPredictionStartTime = 0; // Reset timer to 0
            }


            // Use tf.tidy to ensure Tensors are cleaned up correctly unless marked for saving
            tf.tidy(() => {
                const myInputTensor = tf.browser.fromPixels(myVideoElement)
                    .resizeBilinear([myCurrentInputShape, myCurrentInputShape]) // Use dynamic input shape
                    .div(255.0)
                    .expandDims();
                
                const myResults = LiteRTInterop.runWithTfjsTensors(myModel, myInputTensor);
                const myOutputTensor = myResults[0];
                
                // Normalize output to 0-1 for display
                const myMin = myOutputTensor.min();
                const myMax = myOutputTensor.max();
                const myNormalized = myOutputTensor.sub(myMin).div(myMax.sub(myMin)).squeeze();
                
                // Create RGB tensor for gradient visualization (Blue=Far, Green=Close)
                const myZeros = tf.zerosLike(myNormalized);
                const myBlueChannel = tf.sub(1, myNormalized);
                const myRgbTensor = tf.stack([myZeros, myNormalized, myBlueChannel], 2);
                
                
                // Calculate crop to emulate object-fit: cover
                const videoRatio = myVideoElement.videoWidth / myVideoElement.videoHeight;
                const windowRatio = 640 / 480; 
                const scale = videoRatio / windowRatio;
                
                // Apply scaling via inline CSS
                if (scale > 1) {
                    myOverlayCanvas.style.scale = `${scale} 1`;
                } else {
                    myOverlayCanvas.style.scale = `1 ${1 / scale}`;
                }
                
                // Draw with horizontal flip
                tf.browser.draw(myRgbTensor.clipByValue(0, 1), myOverlayCanvas, { flipHorizontal: true });
                
                for (const output of myResults) {
                    output.dispose();
                }
            });
            
            // Only await submission for subsequent frames once initial compilation is done
            if (myInitialPredictionStartTime === 0) {
                await tf.backend().queue.onSubmittedWorkDone();
            }
            
            requestAnimationFrame(myPredict);
        }

        /**
         * Handles camera changes, stopping the current prediction and restarting the stream.
         */
        async function myCameraChangeHandler() {
            await myEnableWebcam(myCameraSelect.value);
        }

        /**
         * Handles model changes, stopping the current prediction and resetting the UI.
         */
        function myModelChangeHandler() {
            // Stop the current process when a new model is selected.
            // The user must click "Load Model & Start" again for the new model.
            myStopPredictionAndReset();
        }

        // --- Initialization and Event Listeners (Static Links) ---
        
        // Connect the load button to the load function
        myLoadButton.onclick = myLoadModel;
        // Connect the camera change to the handler
        myCameraSelect.onchange = myCameraChangeHandler;
        // Connect the model change to the handler
        myModelSelect.onchange = myModelChangeHandler; // NEW static link

        // Populate the select boxes on script load
        myPopulateModelSelect(); // NEW
        myGetCameras();

        // Initially try to enable the webcam so the user can see the feed before loading the heavy model
        myEnableWebcam();
    </script>
</head>
<body style="margin:0; padding:0; height:100vh; display:flex; flex-direction:column; align-items:center; justify-content:center; background-color:#000;">
    <div id="controls" style="position:relative; z-index:2; margin-bottom:10px; padding:5px; background:rgba(255, 255, 255, 0.9); display:flex; gap:10px; align-items:center;">
        <div>
            <label for="cameraSelect">Camera:</label>
            <select id="cameraSelect" style="margin-right:10px;">
                <option value="">Loading Cameras...</option>
            </select>
        </div>
        
        <div>
            <label for="modelSelect">Model:</label>
            <select id="modelSelect" style="margin-right:10px;">
                </select>
        </div>

        <button id="loadModelButton">Load Model & Start</button>
        
        <div style="font-size:12px; white-space:nowrap;">
            Load Time: <span id="myLoadStatus">Ready</span>
        </div>
        <div style="font-size:12px; white-space:nowrap;">
            Analysis Delay: <span id="myAnalysisStatus">N/A</span>
        </div>
    </div>

    <div style="position:relative; width:640px; height:480px; border: 1px solid white; overflow:hidden;">
        <video id="webcam" autoplay playsinline style="position:absolute; width:100%; height:100%; object-fit:cover; z-index:0;"></video>
        <canvas id="output" style="position:absolute; width:100%; height:100%; z-index:0;"></canvas> 
        <canvas id="overlay" style="position:absolute; top:0; left:0; width:100%; height:100%; pointer-events:none; z-index:1; opacity:0.5;"></canvas>
    </div>
    
    <section id="demos" style="display:none;">
        <h1>LiteRT.js Depth Estimation Demo</h1>
        <p>This demo performs real-time depth estimation.</p>
    </section>
</body>
</html>
