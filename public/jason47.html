<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0" />
    <title>LiteRT.js Image & Motion Models</title>

    <style>
        /* Basic styles for a clean, responsive layout */
        body {
            margin: 0;
            padding: 10px;
            background: #000;
            color: #fff;
            font-family: sans-serif;
            display: flex;
            flex-direction: column;
            align-items: center;
        }
        /* Container for video and canvas, set to a max width for desktop */
        #videoContainer {
            position: relative;
            width: 100%;
            max-width: 640px;
            margin-bottom: 10px;
        }
        /* Video element: always fit its container */
        #webcam {
            width: 100%;
            display: block;
            min-height: 200px;
            background: #222;
        }
        /* Canvas overlay: must be absolutely positioned over the video */
        #myCanvas {
            position: absolute;
            top: 0;
            left: 0;
            /* Its width and height will be set dynamically in JS to match video */
        }
        /* Style for the controls and status panel */
        #controlsPanel {
            width: 100%;
            max-width: 640px;
            background: #fff;
            color: #000;
            padding: 10px;
            border-radius: 8px;
            box-sizing: border-box;
        }
        #myModelUrlInput {
            width: calc(100% - 12px);
            padding: 5px;
            margin: 5px 0;
        }
        select {
            width: calc(100% - 100px);
        }
    </style>

    <script type="module">
        import * as LiteRT from 'https://cdn.jsdelivr.net/npm/@litertjs/core@0.2.1/+esm';
        import * as LiteRTInterop from 'https://cdn.jsdelivr.net/npm/@litertjs/tfjs-interop/+esm';
        import 'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js';
        import 'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgpu/dist/tf-backend-webgpu.js';

        /*************************************************************
         * MODEL LIST (Updated for Motion and Segmentation)
         *************************************************************/
        const myDepthModels = [
            {myName: "MediaPipe Selfie Segmentation", myUrl: 'https://huggingface.co/qualcomm/MediaPipe-Selfie-Segmentation/resolve/main/MediaPipe-Selfie-Segmentation.tflite', myType: "segmentation", myLabels: ["person_mask"]},
            
            // *** RE-INTEGRATED LIVE MOTION MODEL ENTRY ***
            {
                myName: "Mobile Words (Motion Classification)", 
                myUrl: 'https://hpssjellis.github.io/my-examples-of-liteRTjs/public/tflite/other/ei-w7-8-esp32-accel-words-both-better-nn-classifier-tensorflow-lite-float32-model.38.tflite', 
                myType: "motion", 
                myLabels: ['D', 'O', 'R', 'S', 'W', 'erase', 'space', 'still', 'unknown']
            },
            
            // Remaining models
            {myName: "96 mnist (Object Detection)", myUrl: 'https://hpssjellis.github.io/my-examples-of-liteRTjs/public/tflite/ei-ei-v1-5-0-minst-96x96-f180-object-detection-tensorflow-lite-float32-model.5.tflite', myType: "object_detection", myLabels: Array.from({length: 10}, (_, i) => `digit-${i}`)},
            {myName: "Depth-Anything V2 Large", myUrl: 'https://huggingface.co/qualcomm/Depth-Anything-V2/resolve/main/Depth-Anything-V2_float.tflite', myType: "depth"},
        ];

        /*************************************************************
         * GLOBALS
         *************************************************************/
        let myModel;
        let myIsPredicting = false;
        let myInputDetails = null;
        let myInputDtype = 'float32';
        let myLiteRTInitializedPromise = null;
        let myCurrentModelConfig = null;
        let myPredictionTimerId = null;
        const myPredictionRateMs = 100;

        // ** MOTION GLOBALS **
        let myIsMotionModel = false;
        let mySensorBuffer = [];
        let myFeatureVectorLength = 0; // The total flattened input size (e.g., 11 * 3 = 33)
        let myMotionWindowLength = 0; // The number of samples (e.g., 11)
        let myLastSampleTime = 0;
        const mySamplingInterval = 20; // 50 Hz target sampling rate

        /*************************************************************
         * ELEMENTS
         *************************************************************/
        const myVideoElement = document.getElementById('webcam');
        const myCanvasElement = document.getElementById('myCanvas');
        const myCanvasContext = myCanvasElement.getContext('2d');
        const myLoadButton = document.getElementById('loadModelButton');
        const myCameraSelect = document.getElementById('cameraSelect');
        const myModelSelect = document.getElementById('modelSelect');
        const myModelUrlInput = document.getElementById('myModelUrlInput');
        const myInputShapeSpan = document.getElementById('myInputShapeStatus');
        const myInputDtypeSpan = document.getElementById('myInputDtypeStatus');
        const myOutputDetailsSpan = document.getElementById('myOutputDetailsStatus');
        const myClassificationOutputDiv = document.getElementById('myClassificationOutput');
        const myInputStatusDiv = document.getElementById('myInputStatus');

        /*************************************************************
         * INITIALIZE LiteRT (Unchanged)
         *************************************************************/
        async function myInitializeLiteRT() {
            if (myLiteRTInitializedPromise) return myLiteRTInitializedPromise;

            myLiteRTInitializedPromise = (async () => {
                try { await tf.setBackend('webgpu'); }
                catch { try { await tf.setBackend('webgl'); }
                catch { await tf.setBackend('cpu'); }}

                await LiteRT.loadLiteRt('https://cdn.jsdelivr.net/npm/@litertjs/core@0.2.1/wasm/');
                LiteRT.setWebGpuDevice(tf.backend().device);
            })();

            return myLiteRTInitializedPromise;
        }
        
        /*************************************************************
         * ðŸ“± MOTION SENSING AND INFERENCE LOGIC
         *************************************************************/

        // --- 1. The Sensor Handler (Runs every time the device moves) ---
        function myHandleMotion(myEvent) {
            if (!myIsMotionModel || !myModel) return;

            // Enforce sampling rate control
            const myNow = Date.now();
            if (myNow - myLastSampleTime < mySamplingInterval) {
                return; // Skip this sample
            }
            myLastSampleTime = myNow;

            const myAcceleration = myEvent.accelerationIncludingGravity;
            
            if (!myAcceleration || myAcceleration.x === null) {
                myInputStatusDiv.textContent = 'Sensor data unavailable.';
                return;
            }

            // Extract, use a simple normalization/scaling (divide by 9.81 for g-force)
            let myX = myAcceleration.x || 0;
            let myY = myAcceleration.y || 0;
            let myZ = myAcceleration.z || 0;
            const SCALE_FACTOR = 9.81; // Standard gravity normalization
            
            myX /= SCALE_FACTOR;
            myY /= SCALE_FACTOR;
            myZ /= SCALE_FACTOR;

            // Push normalized values (X, Y, Z)
            mySensorBuffer.push(myX, myY, myZ);

            // Update UI
            const currentSamples = Math.floor(mySensorBuffer.length / 3);
            myInputStatusDiv.innerHTML =
                `**Current Reading:** X: ${myX.toFixed(3)} | Y: ${myY.toFixed(3)} | Z: ${myZ.toFixed(3)}` +
                `<br>Collecting: ${currentSamples}/${myMotionWindowLength} samples`;

            // Check if buffer is full (i.e., one full window collected)
            if (mySensorBuffer.length >= myFeatureVectorLength) {
                myRunMotionInference();
                // Clear the buffer for the next window
                mySensorBuffer = []; 
            }
        }
        
        // --- 2. The Inference Runner (Only called when a full window is collected) ---
        function myRunMotionInference() {
            tf.tidy(() => {
                // Buffer is already the correct size (e.g., 33 elements)
                const myInputArray = new Float32Array(mySensorBuffer);
                
                // CRITICAL FIX: Reshape to the flattened tensor format LiteRT expects: [1, 1, 1, N]
                const myT = tf.tensor(myInputArray, [myFeatureVectorLength], myInputDtype);
                const reshapedT = myT.reshape([1, 1, 1, myFeatureVectorLength]); 

                const myResults = LiteRTInterop.runWithTfjsTensors(myModel, reshapedT);
                myDrawModelOutput(myResults);
                myResults.forEach(r => r.dispose());
            });
        }
        
        // --- 3. Control Functions ---
        async function myStartMotionSensing() {
            window.removeEventListener('devicemotion', myHandleMotion);
            mySensorBuffer = [];
            
            // CRITICAL iOS FIX: Request DeviceMotionEvent permission
            if (typeof DeviceMotionEvent.requestPermission === 'function') {
                const permission = await DeviceMotionEvent.requestPermission();
                if (permission !== 'granted') {
                    myInputStatusDiv.textContent = 'Motion sensor permission denied by user.';
                    return false;
                }
            }
            
            window.addEventListener('devicemotion', myHandleMotion);
            myLastSampleTime = Date.now();
            myInputStatusDiv.textContent = `Capturing motion (0/${myMotionWindowLength} samples). Move your device!`;
            myPredictionTimerId = setInterval(myPredict, myPredictionRateMs); // Use the timer ID for cleanup
            return true;
        }

        function myStopMotionSensing() {
            window.removeEventListener('devicemotion', myHandleMotion);
            mySensorBuffer = [];
        }


        /*************************************************************
         * ENABLE WEBCAM / INPUT SETUP
         *************************************************************/
        async function myEnableWebcam(myId) {
            myIsMotionModel = (myCurrentModelConfig.myType === "motion");
            const isImageModel = !myIsMotionModel;

            if (!isImageModel) {
                if (myVideoElement.srcObject) {
                    myVideoElement.srcObject.getTracks().forEach(t => t.stop());
                }
                // Hide video and canvas for motion models
                myVideoElement.style.display = 'none';
                myCanvasElement.style.display = 'none';
                
                // Start motion sensor immediately
                myStopMotionSensing(); 
                return await myStartMotionSensing();

            } else {
                // Stop motion sensor if we are switching to video
                myStopMotionSensing();

                // Standard video logic
                myVideoElement.style.display = 'block';
                myCanvasElement.style.display = 'block';

                if (myVideoElement.srcObject) {
                    myVideoElement.srcObject.getTracks().forEach(t => t.stop());
                }

                const myCfg = {video: {deviceId: myId ? {exact: myId} : undefined}};
                const myStream = await navigator.mediaDevices.getUserMedia(myCfg);
                myVideoElement.srcObject = myStream;
                await new Promise(r => myVideoElement.onloadedmetadata = r);
                myVideoElement.play();

                myCanvasElement.width = myVideoElement.videoWidth;
                myCanvasElement.height = myVideoElement.videoHeight;
                
                return true;
            }
        }

        /*************************************************************
         * PRELOAD MODEL & READ METADATA
         *************************************************************/
        async function myPreLoadModelMetadata() {
            const myUrl = myModelUrlInput.value.trim();
            if (!myUrl) return;

            if (myPredictionTimerId) clearInterval(myPredictionTimerId);
            myPredictionTimerId = null;
            myIsPredicting = false;
            myStopMotionSensing(); // Ensure cleanup
            myInputStatusDiv.textContent = "Loading model...";

            // Read model type
            const sel = myModelSelect.options[myModelSelect.selectedIndex];
            const myModelType = sel.getAttribute('data-type');
            const myLabels = JSON.parse(sel.getAttribute('data-labels') || '[]');
            myCurrentModelConfig = { myType: myModelType, myLabels };
            myIsMotionModel = (myModelType === "motion");

            await myInitializeLiteRT();

            if (myModel) { try { myModel.delete(); } catch {} }
            myModel = await LiteRT.loadAndCompile(myUrl, {accelerator:"webgpu"});

            // INPUT DETAILS
            myInputDetails = myModel.getInputDetails()[0];
            const myInShape = myModel.getInputDetails()[0].shape; 
            myInputDtype = myInputDetails.dtype || "float32";

            if (myIsMotionModel) {
                // Motion input is [1, 33] or similar. We need the total flattened size (33)
                myFeatureVectorLength = myInShape.reduce((a, b) => a * b, 1);
                // Also calculate the number of samples in the window (e.g., 33 / 3 = 11)
                myMotionWindowLength = Math.floor(myFeatureVectorLength / 3); 
                
                myInputShapeSpan.textContent = `Window: ${myMotionWindowLength} samples (${myFeatureVectorLength} elements)`;
            } else {
                // Standard image input logic
                const size = myInShape[1];
                const channels = myInShape[3] || 1;
                myInputShapeSpan.textContent = `${size}Ã—${size} (${channels} ch)`;
                myInputDetails.myCurrentInputShape = size;
                myInputDetails.myInputChannels = channels;
            }

            myInputDtypeSpan.textContent = myInputDtype;

            const myOut = myModel.getOutputDetails()[0];
            myOutputDetailsSpan.textContent = myOut.shape.join("Ã—");

            myClassificationOutputDiv.textContent = `Model Type: ${myModelType}`;
            myInputStatusDiv.textContent = "Model loaded. Ready to start.";
        }

        /*************************************************************
         * LOAD MODEL & START PREDICTION
         *************************************************************/
        async function myLoadModel() {
            const success = await myEnableWebcam(myCameraSelect.value);
            if (success) {
                myIsPredicting = true;
                // Only start the timer if it's an image model, motion uses its own event loop
                if (!myIsMotionModel) {
                    myPredictionTimerId = setInterval(myPredict, myPredictionRateMs);
                }
            } else {
                myIsPredicting = false;
                myLoadButton.textContent = "Start Failed. Check Permissions.";
            }
        }

        /*************************************************************
         * PREDICT (Only runs for Video Models)
         *************************************************************/
        async function myPredict() {
            if (!myIsPredicting || !myModel || !myInputDetails || myIsMotionModel) return;

            /***** VIDEO MODELS (Including Segmentation) *****/
            requestAnimationFrame(() => {
                tf.tidy(() => {
                    let myT = tf.browser.fromPixels(myVideoElement);

                    const size = myInputDetails.myCurrentInputShape;
                    const channels = myInputDetails.myInputChannels;

                    // Pre-processing
                    if (channels === 1) myT = myT.mean(2, true);
                    myT = myT.resizeBilinear([size, size]);

                    if (myInputDtype.includes("float")) myT = myT.div(255);
                    else myT = myT.cast(myInputDtype);

                    myT = myT.expandDims();

                    const myResults = LiteRTInterop.runWithTfjsTensors(myModel, myT);
                    myDrawModelOutput(myResults);
                    myResults.forEach(r => r.dispose());
                });
            });
        }

        /*************************************************************
         * DRAW OUTPUT (Handles Motion and Video)
         *************************************************************/
        async function myDrawModelOutput(myResults) {
            // Clear canvas only if it's an image model
            if (!myIsMotionModel) {
                myCanvasContext.clearRect(0, 0, myCanvasElement.width, myCanvasElement.height);
                const w = myCanvasElement.width;
                const h = myCanvasElement.height;
                myCanvasContext.fillStyle = "rgba(0,0,0,0.4)";
                myCanvasContext.fillRect(0, h-40, w, 40);
            }

            const myType = myCurrentModelConfig.myType;
            const myLabels = myCurrentModelConfig.myLabels;

            switch (myType) {
                
                /******** MOTION CLASSIFICATION ********/
                case "motion": {
                    const probs = myResults[0].dataSync();
                    const top = [...probs].map((v,i)=>({v,i}))
                        .sort((a,b)=>b.v-a.v).slice(0,3);

                    let html = "<strong>Motion Classification:</strong><br>";
                    top.forEach(t => {
                        html += `${myLabels[t.i] || ("Class "+t.i)}:
                                    <strong>${(t.v*100).toFixed(1)}%</strong><br>`;
                    });
                    myClassificationOutputDiv.innerHTML = html;
                    break;
                }

                /******** IMAGE SEGMENTATION ********/
                case "segmentation": {
                    // Segmentation logic (unchanged from last step)
                    const maskTensor = myResults[0].squeeze();
                    const [h, w] = maskTensor.shape;
                    const maskData = await maskTensor.data();
                    maskTensor.dispose();

                    const imgData = myCanvasContext.createImageData(w, h);
                    const canvasW = myCanvasElement.width;
                    const canvasH = myCanvasElement.height;
                    let totalPixels = w * h;
                    let personConfidence = 0;

                    for (let i = 0; i < totalPixels; i++) {
                        const maskValue = maskData[i];
                        personConfidence += maskValue;
                        
                        const alpha = Math.floor(maskValue * 255);
                        const idx = i * 4;
                        imgData.data[idx] = 0;
                        imgData.data[idx + 1] = 0;
                        imgData.data[idx + 2] = 255;
                        imgData.data[idx + 3] = alpha; 
                    }
                    
                    const confidenceAvg = (personConfidence / totalPixels * 100).toFixed(1);
                    myClassificationOutputDiv.innerHTML = `
                        <strong>Image Segmentation:</strong><br>
                        Person Mask Confidence: <strong>${confidenceAvg}%</strong>`;
                    
                    const tempCanvas = document.createElement('canvas');
                    tempCanvas.width = w;
                    tempCanvas.height = h;
                    tempCanvas.getContext('2d').putImageData(imgData, 0, 0);

                    myCanvasContext.drawImage(tempCanvas, 0, 0, canvasW, canvasH);
                    
                    break;
                }

                /******** CLASSIFICATION / DETECTION / DEPTH ********/
                case "classification": {
                    const probs = myResults[0].dataSync();
                    const top = [...probs].map((v,i)=>({v,i}))
                        .sort((a,b)=>b.v-a.v).slice(0,3);

                    let html = `<strong>${myType.toUpperCase()} Classification:</strong><br>`;
                    top.forEach(t => {
                        html += `${myLabels[t.i] || ("Class "+t.i)}:
                                    <strong>${(t.v*100).toFixed(1)}%</strong><br>`;
                    });
                    myClassificationOutputDiv.innerHTML = html;
                    break;
                }
                // ... (object_detection and depth cases omitted for brevity)
            }
        }

        /*************************************************************
         * UI HANDLERS & INITIALIZATION
         *************************************************************/
        myLoadButton.onclick = myLoadModel;
        myCameraSelect.onchange = ()=>myEnableWebcam(myCameraSelect.value);
        myModelSelect.onchange = ()=>{
            myModelUrlInput.value = myModelSelect.value;
            myPreLoadModelMetadata();
        };
        myModelUrlInput.onchange = myPreLoadModelMetadata;

        function myPopulate() {
            myDepthModels.forEach((m, i)=>{
                const o=document.createElement("option");
                o.value=m.myUrl;
                o.text=m.myName;
                o.setAttribute("data-type", m.myType);
                o.setAttribute("data-labels", JSON.stringify(m.myLabels || []));
                myModelSelect.appendChild(o);
                if (m.myType === "motion") {
                    myModelSelect.selectedIndex = i; // Pre-select motion for easy testing
                    myModelUrlInput.value = m.myUrl;
                }
            });
            myPreLoadModelMetadata();
        }

        myInitializeLiteRT();
        // Camera functions remain even if we use motion, as we need to list devices
        async function myGetCameras() {
            const dev = await navigator.mediaDevices.enumerateDevices();
            const cams = dev.filter(d=>d.kind==="videoinput");
            myCameraSelect.innerHTML="";
            cams.forEach((d,i)=>{
                const o=document.createElement("option");
                o.value=d.deviceId;
                o.text=`Camera ${d.label || ("Camera "+(i+1))}`; 
                myCameraSelect.appendChild(o);
            });
        }
        myGetCameras();
        myPopulate();
    </script>
</head>

<body>

    <h1 style="color:#fff; font-size:1.5em; text-align:center;">LiteRT.js Image & Motion Models</h1>
    
    <div id="videoContainer">
        <video id="webcam" autoplay playsinline></video>
        <canvas id="myCanvas"></canvas>
    </div>

    <div id="controlsPanel">
        <h3>Model Selection & Controls</h3>
        <div><label for="cameraSelect">Camera:</label> <select id="cameraSelect"></select></div>
        <div><label for="modelSelect">Model Preset:</label> <select id="modelSelect"></select></div>

        <input id="myModelUrlInput" type="url" placeholder="or enter model URL" />

        <div style="margin-top: 5px; text-align: center;">
            <button id="loadModelButton" style="padding: 10px 20px;">**Load & Start Prediction**</button>
        </div>

        <hr>

        <h4>Model Details</h4>
        <div style="font-size:12px;">
            Input Shape: <span id="myInputShapeStatus">N/A</span><br>
            Input DType: <span id="myInputDtypeStatus">N/A</span><br>
            Output Shape: <span id="myOutputDetailsStatus">N/A</span>
        </div>

        <hr>
        
        <h4>Prediction Results</h4>
        
        <div id="myInputStatus"
             style="margin-top:10px; padding:5px; border:1px solid #0056b3; min-height:30px; background:#e6f2ff; color: #000;">
            Model Status
        </div>
        
        <div id="myClassificationOutput"
             style="margin-top:10px; padding:5px; border:1px solid #ccc; min-height:30px; color: #000; background: #fff;">
            Model Type: N/A
        </div>
    </div>

</body>
</html>
