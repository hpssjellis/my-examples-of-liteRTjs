<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>myWakeWordDetector</title>

    <style>
        /* General Layout */
        body { 
            font-family: Arial, sans-serif; 
            display: flex; 
            flex-direction: column; 
            align-items: center; 
            margin: 20px; 
            background-color: #f0f0f0; 
        }
        #myContainer { 
            max-width: 480px; 
            width: 100%; 
            background: white; 
            padding: 20px; 
            border-radius: 8px; 
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); 
            margin-bottom: 20px;
        }
        /* Buttons */
        .myButton { 
            background-color: #008080; /* Teal */
            color: white; 
            padding: 10px 15px; 
            border: none; 
            border-radius: 4px; 
            cursor: pointer; 
            font-size: 16px; 
            margin-top: 10px; 
            width: 100%; 
            box-sizing: border-box;
            transition: background-color 0.3s;
        }
        .myButton:hover { 
            background-color: #006666; 
        }
        .myButton:disabled { 
            background-color: #ccc; 
            cursor: not-allowed; 
        }
        /* Status and Input */
        .myStatus { 
            margin-top: 10px; 
            font-weight: bold; 
            color: #333; 
            text-align: center; 
            padding: 10px; 
            border-radius: 4px; 
            background-color: #f0ffff;
            border: 1px solid #b3cccc;
        }
        .myInput { 
            width: 100%; 
            padding: 8px; 
            border: 1px solid #ccc; 
            border-radius: 4px; 
            box-sizing: border-box; 
        }
        .myGroup { 
            border: 1px dashed #bbb; 
            padding: 15px; 
            border-radius: 4px; 
            margin-top: 15px; 
        }
        #myPredictionResult {
            text-align: center;
            font-size: 1.5em;
            font-weight: bold;
            margin-top: 15px;
            min-height: 40px;
            color: #008080;
        }
        #myWaveform {
            width: 100%;
            height: 50px;
            border: 1px solid #ddd;
            border-radius: 4px;
            margin-top: 10px;
            background-color: #eee;
        }
    </style>
    
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.17.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite@0.0.1-alpha.9/dist/tf-tflite.min.js"></script>
</head>
<body>

    <script>
        // =================================================================
        // 🚀 USER-CONFIGURABLE MODEL SETTINGS 
        // =================================================================
        const myModelConfig = {
            // Default URL for the wake word model
            myDefaultUrl: './tflite/ei-ei-v202-xiao-sounds-0unknown-1no-2yes-esp32-nn-classifier-tensorflow-lite-float32-model.12.tflite', 
            
            // Expected sample rate of the model's audio data
            myTargetSampleRate: 16000, 
            
            // Raw audio buffer length (1 second of 16kHz audio)
            myWindowSizeSamples: 16000, 
            
            // The final feature vector size the model expects (the fixed 260 input)
            myFeatureVectorLength: 260, 
            
            // Labels matching the model's output index order
            myLabels: ['_background_noise_', 'No', 'Yes'], 
        };

        // =================================================================
        // ⚙️ INTERNAL VARIABLES 
        // =================================================================
        let myTfLiteModel = null;
        let myAudioContext = null;
        let myStream = null;
        let myScriptProcessor = null;
        let myRawAudioBuffer = []; // Buffer for collecting raw samples at browser rate
        let myFeatureBuffer = null; // Float32Array for 260 features
        let mySamplesCollected = 0;
        
        let myBrowserRate = 0; // The actual sample rate of the browser's mic
        let myResampleRatio = 0; // Ratio for downsampling to 16000Hz

        let myStatusElement = null;
        let myPredictionElement = null;
        let myFileInput = null;
        let myUrlInput = null;
        let myWaveformCanvas = null;
        let myWaveformContext = null;

        // =================================================================
        // 🏠 HTML SETUP (No Changes)
        // =================================================================
        
        function myCreateDOM() {
            const myContainer = document.createElement('div');
            myContainer.id = 'myContainer';
            
            const myTitle = document.createElement('h1');
            myTitle.style.fontSize = '1.5em';
            myTitle.style.fontWeight = 'bold';
            myTitle.style.marginBottom = '15px';
            myTitle.style.textAlign = 'center';
            myTitle.textContent = 'Edge Impulse Wake Word Detector';
            myContainer.appendChild(myTitle);

            myStatusElement = document.createElement('p');
            myStatusElement.id = 'myStatusMessage';
            myStatusElement.className = 'myStatus';
            myStatusElement.textContent = 'Select a model loading method to start.';
            myContainer.appendChild(myStatusElement);
            
            myWaveformCanvas = document.createElement('canvas');
            myWaveformCanvas.id = 'myWaveform';
            myWaveformCanvas.width = 440;
            myWaveformCanvas.height = 50;
            myContainer.appendChild(myWaveformCanvas);
            myWaveformContext = myWaveformCanvas.getContext('2d');
            
            const myControlsDiv = document.createElement('div');
            myControlsDiv.id = 'myControls';

            const myFileGroup = document.createElement('div');
            myFileGroup.className = 'myGroup';
            myFileGroup.style.borderColor = '#008080';
            myFileGroup.style.backgroundColor = '#f0faff';

            const myFileLabel = document.createElement('p');
            myFileLabel.style.fontWeight = 'bold';
            myFileLabel.textContent = `1. Load From Computer File (.tflite):`;
            myFileGroup.appendChild(myFileLabel);
            
            myFileInput = document.createElement('input'); 
            myFileInput.type = 'file';
            myFileInput.id = 'myFileInput';
            myFileInput.accept = '.tflite';
            myFileInput.style.marginTop = '10px';
            myFileInput.onchange = myLoadFromFile; 
            myFileGroup.appendChild(myFileInput);
            myControlsDiv.appendChild(myFileGroup);

            const myDefaultUrlButton = document.createElement('button');
            myDefaultUrlButton.className = 'myButton';
            myDefaultUrlButton.textContent = `2. Load Default URL (${myModelConfig.myDefaultUrl.split('/').pop()})`;
            myDefaultUrlButton.onclick = myLoadFromDefaultUrl;
            myControlsDiv.appendChild(myDefaultUrlButton);

            const myUrlGroup = document.createElement('div');
            myUrlGroup.className = 'myGroup';

            const myUrlLabel = document.createElement('p');
            myUrlLabel.style.fontWeight = 'bold';
            myUrlLabel.textContent = '3. Load From Custom URL:';
            myUrlGroup.appendChild(myUrlLabel);

            myUrlInput = document.createElement('input');
            myUrlInput.type = 'text';
            myUrlInput.id = 'myUrlInput';
            myUrlInput.value = myModelConfig.myDefaultUrl; 
            myUrlInput.placeholder = 'Enter .tflite URL here';
            myUrlInput.className = 'myInput';
            myUrlInput.style.marginTop = '8px';
            myUrlGroup.appendChild(myUrlInput);

            const myCustomUrlButton = document.createElement('button');
            myCustomUrlButton.className = 'myButton';
            myCustomUrlButton.textContent = 'Load Custom URL and Start Microphone';
            myCustomUrlButton.onclick = myLoadFromCustomUrl;
            myUrlGroup.appendChild(myCustomUrlButton);
            
            myControlsDiv.appendChild(myUrlGroup);

            const myResetButton = document.createElement('button');
            myResetButton.className = 'myButton';
            myResetButton.textContent = 'Reset / Stop Microphone';
            myResetButton.onclick = myResetApplication;
            myResetButton.style.marginTop = '25px'; 
            myControlsDiv.appendChild(myResetButton);
            
            myContainer.appendChild(myControlsDiv);
            document.body.appendChild(myContainer);

            myPredictionElement = document.createElement('p');
            myPredictionElement.id = 'myPredictionResult';
            myPredictionElement.textContent = 'Inference not started.';
            document.body.appendChild(myPredictionElement);
        }
        
        window.onload = myCreateDOM; 

        // =================================================================
        // 💻 MODEL LOADING AND STARTUP LOGIC (No Changes)
        // =================================================================
        
        function myGetAllControls() {
            return [
                ...document.querySelectorAll('.myButton'),
                myFileInput,
                myUrlInput
            ];
        }

        function myDisableControls(myDisabled) {
            myGetAllControls().forEach(myEl => myEl.disabled = myDisabled);
        }

        async function myLoadFromDefaultUrl() {
            await myLoadModelAndStartMic(myModelConfig.myDefaultUrl);
        }

        async function myLoadFromCustomUrl() {
            const myCustomUrl = myUrlInput.value.trim();
            if (myCustomUrl) {
                await myLoadModelAndStartMic(myCustomUrl);
            } else {
                myStatusElement.textContent = 'Error: Please enter a valid URL in the custom URL field.';
            }
        }

        async function myLoadFromFile() {
            const mySelectedFile = myFileInput.files[0];
            if (mySelectedFile) {
                await myLoadModelAndStartMic(mySelectedFile);
            } else {
                myDisableControls(false); 
                myStatusElement.textContent = 'Please select a .tflite file first or choose another method.';
            }
        }

        async function myLoadModelAndStartMic(mySource) {
            myStopAudioProcessing(); 
            myDisableControls(true);

            const mySourceType = typeof mySource === 'string' ? 'URL' : 'File';
            const myDisplayPath = typeof mySource === 'string' ? mySource : mySource.name;
            
            myStatusElement.textContent = `Loading model from ${mySourceType}: ${myDisplayPath}...`;
            
            let myModelSource = mySource; 

            try {
                if (mySourceType === 'File') {
                    myModelSource = await new Promise((myResolve, myReject) => {
                        const myReader = new FileReader();
                        myReader.onload = () => myResolve(myReader.result);
                        myReader.onerror = myReject;
                        myReader.readAsArrayBuffer(mySource);
                    });
                    myStatusElement.textContent = `File read into buffer. Loading model...`;
                }

                myTfLiteModel = await tflite.loadTFLiteModel(myModelSource);
                
                myStatusElement.textContent = 'Model loaded successfully! Initializing microphone...';
                console.log(`TFLite Model Loaded from ${mySourceType}: ${myDisplayPath}`);

                await mySetupMicrophone();

            } catch (myError) {
                myStatusElement.textContent = `Failed to load model from ${mySourceType}. Check the console (F12) for details.`;
                console.error('Model loading error:', myError);
                
                myDisableControls(false);
            }
        }

        // =================================================================
        // 🎤 MICROPHONE AND AUDIO PROCESSING (IMPROVED)
        // =================================================================

        /**
         * @function mySetupMicrophone
         * Initializes the Web Audio API and starts mic stream, calculating the resample ratio.
         */
        async function mySetupMicrophone() {
            try {
                if (!myAudioContext) {
                    myAudioContext = new (window.AudioContext || window.webkitAudioContext)();
                }
                if (myAudioContext.state === 'suspended') {
                    await myAudioContext.resume();
                }

                myStream = await navigator.mediaDevices.getUserMedia({ audio: true, video: false });
                const myMicSource = myAudioContext.createMediaStreamSource(myStream);
                
                // Get the actual browser rate
                myBrowserRate = myAudioContext.sampleRate;
                
                // Calculate the ratio needed to downsample to the model's target rate (16000 Hz)
                myResampleRatio = myBrowserRate / myModelConfig.myTargetSampleRate; 

                // We target collecting enough raw samples for 1 second at the browser's rate
                mySamplesCollected = 0;
                
                myScriptProcessor = myAudioContext.createScriptProcessor(4096, 1, 1); 
                myScriptProcessor.onaudioprocess = myProcessAudioBuffer;

                myMicSource.connect(myScriptProcessor);
                myScriptProcessor.connect(myAudioContext.destination);

                myStatusElement.textContent = `Mic active (Browser Rate: ${myBrowserRate}Hz). Target Rate: ${myModelConfig.myTargetSampleRate}Hz. Listening...`;
                myDisableControls(false); 

            } catch (myError) {
                myStatusElement.textContent = `Error accessing microphone: ${myError.message}.`;
                console.error('Microphone access error:', myError);
                myDisableControls(false);
            }
        }
        
        /**
         * @function myResampleAndCollect
         * Downsamples the raw chunk and adds it to the buffer until 16000 samples are collected.
         * @param {Float32Array} myInputBuffer - The chunk of raw audio from the mic.
         */
        function myResampleAndCollect(myInputBuffer) {
            
            // Simple Resampling: take every Nth sample
            for (let i = 0; i < myInputBuffer.length; i++) {
                // If the index is a multiple of the resample ratio, keep the sample
                if (i % myResampleRatio < 1) { // Checks if i / ratio is close to an integer
                    myRawAudioBuffer.push(myInputBuffer[i]);
                }
            }

            // Update the count of collected 16kHz samples
            mySamplesCollected = myRawAudioBuffer.length;

            // Check if we have collected at least 16000 resampled samples
            if (mySamplesCollected >= myModelConfig.myWindowSizeSamples) {
                // Trim to exactly 16000 samples
                const myWindow = myRawAudioBuffer.slice(0, myModelConfig.myWindowSizeSamples);
                
                // FIXED STEP: Generate the improved 260 features
                myFeatureBuffer = myGenerateAudioFeatures(myWindow);
                
                myRunInference();
                
                // Keep the excess data for the next window for overlapping processing
                myRawAudioBuffer = myRawAudioBuffer.slice(myModelConfig.myWindowSizeSamples);
                mySamplesCollected = myRawAudioBuffer.length;
            }
        }

        /**
         * @function myProcessAudioBuffer
         * The core audio loop.
         */
        function myProcessAudioBuffer(myAudioEvent) {
            if (!myTfLiteModel || myAudioContext.state !== 'running') return;

            const myInputBuffer = myAudioEvent.inputBuffer.getChannelData(0); // Get mono data

            myDrawWaveform(myInputBuffer); // Draw the raw waveform
            
            // Resample the raw chunk and collect it
            myResampleAndCollect(myInputBuffer); 

            // Update status
            myStatusElement.textContent = `Mic active. 16kHz Samples collected: ${mySamplesCollected}/${myModelConfig.myWindowSizeSamples}`;
        }
        
        /**
         * @function myGenerateAudioFeatures
         * IMPROVED: Condenses the 16000 raw audio samples into the required 260 features 
         * using RMS Energy and Zero-Crossing Rate.
         * @param {Array<number>} myRawBuffer - The collected raw audio data (exactly 16000 samples).
         * @returns {Float32Array} The concatenated 260-feature vector.
         */
        function myGenerateAudioFeatures(myRawBuffer) {
            const myRawLength = myRawBuffer.length; // Should be 16000
            
            // Feature Configuration
            const myNumFeatures = myModelConfig.myFeatureVectorLength / 2; // 130 windows for RMS, 130 for ZCR
            const myWindowLength = Math.floor(myRawLength / myNumFeatures); // 16000 / 130 = 123 samples per window
            const myStepSize = Math.floor(myWindowLength / 2); // 50% overlap (61 samples)

            const myRmsFeatures = [];
            const myZcrFeatures = [];
            
            for (let i = 0; i < myNumFeatures; i++) {
                const myStart = i * myStepSize;
                const myEnd = Math.min(myRawLength, myStart + myWindowLength);
                
                let myEnergySum = 0;
                let myZcrCount = 0;
                let myPrevSign = myRawBuffer[myStart] >= 0;

                for (let j = myStart; j < myEnd; j++) {
                    const mySample = myRawBuffer[j];
                    
                    // 1. RMS Calculation
                    myEnergySum += mySample * mySample;
                    
                    // 2. ZCR Calculation
                    const myCurrentSign = mySample >= 0;
                    if (myCurrentSign !== myPrevSign) {
                        myZcrCount++;
                    }
                    myPrevSign = myCurrentSign;
                }
                
                // Store RMS (Root Mean Square: sqrt(mean(squared amplitude)))
                const myRMS = myEnd > myStart ? Math.sqrt(myEnergySum / (myEnd - myStart)) : 0;
                myRmsFeatures.push(myRMS);
                
                // Store ZCR (Normalized Zero-Crossing Rate: crossings per sample)
                const myZCR = myEnd > myStart ? myZcrCount / (myEnd - myStart) : 0;
                myZcrFeatures.push(myZCR);
            }
            
            // Concatenate RMS (130) and ZCR (130) to get 260 features
            const myFeatureArray = new Float32Array(myRmsFeatures.concat(myZcrFeatures));

            console.log(`IMPROVED: Generated feature vector of length ${myFeatureArray.length} from ${myRawLength} 16kHz samples.`);
            return myFeatureArray;
        }

        // =================================================================
        // 📉 VISUALIZATION AND INFERENCE (Minimal Changes)
        // =================================================================
        
        function myDrawWaveform(myBuffer) {
            const myWidth = myWaveformCanvas.width;
            const myHeight = myWaveformCanvas.height;
            const myCtx = myWaveformContext;

            myCtx.clearRect(0, 0, myWidth, myHeight);
            myCtx.fillStyle = '#f0faff';
            myCtx.fillRect(0, 0, myWidth, myHeight);
            myCtx.strokeStyle = '#008080';
            myCtx.lineWidth = 1;

            myCtx.beginPath();
            const mySliceWidth = myWidth * 1.0 / myBuffer.length;
            let x = 0;

            for(let i = 0; i < myBuffer.length; i++) {
                const v = myBuffer[i];
                const y = myHeight / 2 + v * (myHeight / 2);

                if(i === 0) {
                    myCtx.moveTo(x, y);
                } else {
                    myCtx.lineTo(x, y);
                }
                x += mySliceWidth;
            }

            myCtx.lineTo(myWidth, myHeight / 2);
            myCtx.stroke();
        }

        /**
         * @function myRunInference
         * Runs the TFLite model on the 260-feature vector.
         */
        function myRunInference() {
            tf.tidy(() => {
                if (!myTfLiteModel || !myFeatureBuffer) return;
                
                // 1. Prepare Input Tensor (using the 260-element feature buffer)
                const myInputArray = myFeatureBuffer;
                
                // Reshape to [1, 260] 
                const myInputTensor = tf.tensor(myInputArray).reshape([1, myModelConfig.myFeatureVectorLength]); 

                console.log('Input tensor shape:', myInputTensor.shape);

                // 2. Run Inference
                const myOutputTensor = myTfLiteModel.predict(myInputTensor);

                // 3. Post-processing (Classification)
                myPostProcessClassification(myOutputTensor);
            }); 
        }
        
        function myPostProcessClassification(myOutputTensor) {
            const myProbabilities = myOutputTensor.dataSync();
            const myBestIndex = myProbabilities.indexOf(Math.max(...myProbabilities));
            const myBestLabel = myModelConfig.myLabels[myBestIndex];
            const myConfidence = myProbabilities[myBestIndex];

            let myDisplayResult;
            let myDisplayColor;

            if (myBestLabel === 'Yes' && myConfidence > 0.75) {
                myDisplayResult = `WAKE WORD: ${myBestLabel.toUpperCase()} DETECTED! (${(myConfidence * 100).toFixed(1)}%)`;
                myDisplayColor = '#e53e3e'; 
            } else {
                myDisplayResult = `${myBestLabel} (${(myConfidence * 100).toFixed(1)}%)`;
                myDisplayColor = '#008080'; 
            }
            
            console.log(
                `Prediction: ${myBestLabel} | Confidence: ${myConfidence.toFixed(4)}`
            );
            
            myPredictionElement.textContent = myDisplayResult;
            myPredictionElement.style.color = myDisplayColor;
        }
        
        // =================================================================
        // 🧹 RESET FUNCTIONS (Minimal Changes)
        // =================================================================
        
        function myResetApplication() {
            myStopAudioProcessing();
            myTfLiteModel = null;
            
            myPredictionElement.textContent = 'Inference not started.';
            myPredictionElement.style.color = '#008080';
            myStatusElement.textContent = 'Application Reset. Select a model loading method to start.';
            myDisableControls(false); 
            console.log('Wake Word Detection app reset.');
        }

        function myStopAudioProcessing() {
            if (myStream) {
                myStream.getTracks().forEach(track => track.stop());
                myStream = null;
            }
            if (myAudioContext && myAudioContext.state !== 'closed') {
                myAudioContext.close();
                myAudioContext = null;
            }
            myRawAudioBuffer = [];
            myFeatureBuffer = null;
            mySamplesCollected = 0;
            if (myWaveformContext && myWaveformCanvas) {
                myWaveformContext.clearRect(0, 0, myWaveformCanvas.width, myWaveformCanvas.height);
            }
        }
    </script>
</body>
</html>
