<!DOCTYPE html>
<html lang="en">
<head>
Â  Â  <meta charset="UTF-8" />
Â  Â  <meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0" />
Â  Â  <title>LiteRT.js Multi-Model Visualizer â€” Fixed Hybrid Version</title>

Â  Â  <style>
Â  Â  Â  Â  /* Basic styles for a clean, responsive layout */
Â  Â  Â  Â  body {
Â  Â  Â  Â  Â  Â  margin: 0;
Â  Â  Â  Â  Â  Â  padding: 10px;
Â  Â  Â  Â  Â  Â  background: #000;
Â  Â  Â  Â  Â  Â  color: #fff;
Â  Â  Â  Â  Â  Â  font-family: sans-serif;
Â  Â  Â  Â  Â  Â  display: flex; /* Use flexbox for a better mobile/desktop experience */
Â  Â  Â  Â  Â  Â  flex-direction: column;
Â  Â  Â  Â  Â  Â  align-items: center; /* Center content horizontally */
Â  Â  Â  Â  }
Â  Â  Â  Â  /* Container for video and canvas, set to a max width for desktop */
Â  Â  Â  Â  #videoContainer {
Â  Â  Â  Â  Â  Â  position: relative;
Â  Â  Â  Â  Â  Â  width: 100%; /* Take full width of parent */
Â  Â  Â  Â  Â  Â  max-width: 640px; /* Optional: limit width on large screens */
Â  Â  Â  Â  Â  Â  margin-bottom: 10px;
Â  Â  Â  Â  }
Â  Â  Â  Â  /* Video element: always fit its container */
Â  Â  Â  Â  #webcam {
Â  Â  Â  Â  Â  Â  width: 100%;
Â  Â  Â  Â  Â  Â  display: block; /* Removes any extra space below the video */
Â  Â  Â  Â  Â  Â  min-height: 200px; /* Ensure video container shows up even when hidden */
Â  Â  Â  Â  Â  Â  background: #222;
Â  Â  Â  Â  }
Â  Â  Â  Â  /* Canvas overlay: must be absolutely positioned over the video */
Â  Â  Â  Â  #myCanvas {
Â  Â  Â  Â  Â  Â  position: absolute;
Â  Â  Â  Â  Â  Â  top: 0;
Â  Â  Â  Â  Â  Â  left: 0;
Â  Â  Â  Â  Â  Â  /* Its width and height will be set dynamically in JS to match video */
Â  Â  Â  Â  }
Â  Â  Â  Â  /* Style for the controls and status panel */
Â  Â  Â  Â  #controlsPanel {
Â  Â  Â  Â  Â  Â  width: 100%;
Â  Â  Â  Â  Â  Â  max-width: 640px; /* Match the max-width of the video container */
Â  Â  Â  Â  Â  Â  background: #fff;
Â  Â  Â  Â  Â  Â  color: #000;
Â  Â  Â  Â  Â  Â  padding: 10px;
Â  Â  Â  Â  Â  Â  border-radius: 8px;
Â  Â  Â  Â  Â  Â  box-sizing: border-box; /* Include padding in the element's total width and height */
Â  Â  Â  Â  }
Â  Â  Â  Â  #myModelUrlInput {
Â  Â  Â  Â  Â  Â  width: calc(100% - 12px); /* Calc to account for padding */
Â  Â  Â  Â  Â  Â  padding: 5px;
Â  Â  Â  Â  Â  Â  margin: 5px 0;
Â  Â  Â  Â  }
Â  Â  Â  Â  select {
Â  Â  Â  Â  Â  Â  width: calc(100% - 100px); /* Adjust select width to fit next to label */
Â  Â  Â  Â  }
Â  Â  Â  Â  /* Color for the motion/anomaly box */
Â  Â  Â  Â  #myMotionOutput {
Â  Â  Â  Â  Â  Â  background:#e0ffff; /* Light blue for sensor data */
Â  Â  Â  Â  Â  Â  color: #000;
Â  Â  Â  Â  }
Â  Â  Â  Â  /* Color for the general classification output box */
Â  Â  Â  Â  #myClassificationOutput {
Â  Â  Â  Â  Â  Â  background: #f0f0ff; /* Light purple/grey */
Â  Â  Â  Â  Â  Â  color: #000;
Â  Â  Â  Â  }
Â  Â  </style>

Â  Â  <script type="module">
Â  Â  Â  Â  import * as LiteRT from 'https://cdn.jsdelivr.net/npm/@litertjs/core@0.2.1/+esm';
Â  Â  Â  Â  import * as LiteRTInterop from 'https://cdn.jsdelivr.net/npm/@litertjs/tfjs-interop/+esm';
Â  Â  Â  Â  import 'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js';
Â  Â  Â  Â  import 'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgpu/dist/tf-backend-webgpu.js';

Â  Â  Â  Â  /*************************************************************
Â  Â  Â  Â  Â * MODEL LIST (Updated for Motion/Anomaly)
Â  Â  Â  Â  Â *************************************************************/
Â  Â  Â  Â  const myDepthModels = [
Â  Â  Â  Â  Â  Â  // Motion Models
Â  Â  Â  Â  Â  Â  {myName: "Cell Phone Motion (Motion Classification)", myUrl: 'https://hpssjellis.github.io/my-examples-of-liteRTjs/public/tflite/other/ei-w7-8-esp32-accel-words-both-better-nn-classifier-tensorflow-lite-float32-model.38.tflite', myType: "motion", myLabels: ['D', 'O', 'R', 'S', 'W', 'erase', 'space', 'still', 'unknown']},
Â  Â  Â  Â  Â  Â  {myName: "Anomaly Motion (Magnitude only)", myUrl: 'https://hpssjellis.github.io/my-examples-of-liteRTjs/public/tflite/ei-ei-v1-motion-anamoly-still-classifier-tensorflow-lite-float32-model.15.tflite', myType: "anomaly", myLabels: ["anomaly_score"]},
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  // Visual Models
Â  Â  Â  Â  Â  Â  {myName: "96 mnist (Object Detection)", myUrl: 'https://hpssjellis.github.io/my-examples-of-liteRTjs/public/tflite/ei-ei-v1-5-0-minst-96x96-f180-object-detection-tensorflow-lite-float32-model.5.tflite', myType: "object_detection", myLabels: Array.from({length: 10}, (_, i) => `${i}`)},
Â  Â  Â  Â  Â  Â  {myName: "Fomo (Object Detection)", myUrl: 'https://hpssjellis.github.io/my-examples-of-liteRTjs/public/tflite/ei-ei-andrew-3d-printed-symbol-fomo-object-detection-tensorflow-lite-float32-model.5.tflite', myType: "object_detection", myLabels: ["1", "2"]},
Â  Â  Â  Â  Â  Â  {myName: "Depth-Anything V2 Large", myUrl: 'https://huggingface.co/qualcomm/Depth-Anything-V2/resolve/main/Depth-Anything-V2_float.tflite', myType: "depth"},
Â  Â  Â  Â  Â  Â  {myName: "fomo pen", myUrl: 'https://hpssjellis.github.io/my-examples-of-liteRTjs/public/tflite/ei-ei-fomo-pen-object-detection-tensorflow-lite-float32-model.5.tflite', myType: "object_detection", myLabels: ["pen", "other"]},
Â  Â  Â  Â  Â  Â  {myName: "Brush (Classification)", myUrl: 'https://hpssjellis.io/my-examples-of-liteRTjs/public/tflite/ei-ei-jeremy-0unknown-1brush-2paint-v01-transfer-learning-tensorflow-lite-float32-model.5.tflite', myType: "classification", myLabels: ["unknown", "brush", "paint"]},
Â  Â  Â  Â  ];

Â  Â  Â  Â  /*************************************************************
Â  Â  Â  Â  Â * GLOBALS
Â  Â  Â  Â  Â *************************************************************/
Â  Â  Â  Â  let myModel;
Â  Â  Â  Â  let myIsPredicting = false;
Â  Â  Â  Â  let myInputDetails = null;
Â  Â  Â  Â  let myInputDtype = 'float32';
Â  Â  Â  Â  let myLiteRTInitializedPromise = null;
Â  Â  Â  Â  let myCurrentModelConfig = null;
Â  Â  Â  Â  // Timer for the MOTION loop (setInterval)
Â  Â  Â  Â  let myPredictionTimerId = null; 
Â  Â  Â  Â  const myPredictionRateMs = 200;
Â  Â  Â  Â  // Flag for the VISUAL loop (requestAnimationFrame)
Â  Â  Â  Â  let myVisualLoopRunning = false; 

Â  Â  Â  Â  // ðŸš€ Motion Globals
Â  Â  Â  Â  let myIsMotionOrAnomalyModel = false;
Â  Â  Â  Â  let myMotionData = [];
Â  Â  Â  Â  let myMotionListener;
Â  Â  Â  Â  let myMotionSequenceLength = 0;
Â  Â  Â  Â  let myLastStatusUpdate = 0;
Â  Â  Â  Â  const mySamplingIntervalMs = 20; // 50 Hz target sampling rate
Â  Â  Â  Â  let myLastSampleTime = 0;

Â  Â  Â  Â  /*************************************************************
Â  Â  Â  Â  Â * ELEMENTS
Â  Â  Â  Â  Â *************************************************************/
Â  Â  Â  Â  const myVideoElement = document.getElementById('webcam');
Â  Â  Â  Â  const myCanvasElement = document.getElementById('myCanvas');
Â  Â  Â  Â  const myCanvasContext = myCanvasElement.getContext('2d');
Â  Â  Â  Â  const myLoadButton = document.getElementById('loadModelButton');
Â  Â  Â  Â  const myCameraSelect = document.getElementById('cameraSelect');
Â  Â  Â  Â  const myModelSelect = document.getElementById('modelSelect');
Â  Â  Â  Â  const myModelUrlInput = document.getElementById('myModelUrlInput');
Â  Â  Â  Â  const myInputShapeSpan = document.getElementById('myInputShapeStatus');
Â  Â  Â  Â  const myInputDtypeSpan = document.getElementById('myInputDtypeStatus');
Â  Â  Â  Â  const myOutputDetailsSpan = document.getElementById('myOutputDetailsStatus');
Â  Â  Â  Â  const myClassificationOutputDiv = document.getElementById('myClassificationOutput');
Â  Â  Â  Â  const myMotionOutputDiv = document.getElementById('myMotionOutput');

Â  Â  Â  Â  /*************************************************************
Â  Â  Â  Â  Â * INITIALIZE LiteRT
Â  Â  Â  Â  Â *************************************************************/
Â  Â  Â  Â  async function myInitializeLiteRT() {
Â  Â  Â  Â  Â  Â  if (myLiteRTInitializedPromise) return myLiteRTInitializedPromise;

Â  Â  Â  Â  Â  Â  myLiteRTInitializedPromise = (async () => {
Â  Â  Â  Â  Â  Â  Â  Â  try { await tf.setBackend('webgpu'); }
Â  Â  Â  Â  Â  Â  Â  Â  catch { try { await tf.setBackend('webgl'); }
Â  Â  Â  Â  Â  Â  Â  Â  catch { await tf.setBackend('cpu'); }}

Â  Â  Â  Â  Â  Â  Â  Â  await LiteRT.loadLiteRt('https://cdn.jsdelivr.net/npm/@litertjs/core@0.2.1/wasm/');
Â  Â  Â  Â  Â  Â  Â  Â  LiteRT.setWebGpuDevice(tf.backend().device);
Â  Â  Â  Â  Â  Â  })();

Â  Â  Â  Â  Â  Â  return myLiteRTInitializedPromise;
Â  Â  Â  Â  }

Â  Â  Â  Â  /*************************************************************
Â  Â  Â  Â  Â * ENABLE WEBCAM
Â  Â  Â  Â  Â *************************************************************/
Â  Â  Â  Â  async function myEnableWebcam(myId) {
Â  Â  Â  Â  Â  Â  if (myIsMotionOrAnomalyModel) {
Â  Â  Â  Â  Â  Â  Â  Â  if (myVideoElement.srcObject) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myVideoElement.srcObject.getTracks().forEach(t => t.stop());
Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  myVideoElement.style.display = 'none';
Â  Â  Â  Â  Â  Â  Â  Â  myCanvasElement.style.display = 'none';
Â  Â  Â  Â  Â  Â  Â  Â  return;
Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  myVideoElement.style.display = 'block';
Â  Â  Â  Â  Â  Â  myCanvasElement.style.display = 'block';

Â  Â  Â  Â  Â  Â  if (myVideoElement.srcObject) {
Â  Â  Â  Â  Â  Â  Â  Â  myVideoElement.srcObject.getTracks().forEach(t => t.stop());
Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  const myCfg = {video: {deviceId: myId ? {exact: myId} : undefined}};
Â  Â  Â  Â  Â  Â  const myStream = await navigator.mediaDevices.getUserMedia(myCfg);
Â  Â  Â  Â  Â  Â  myVideoElement.srcObject = myStream;
Â  Â  Â  Â  Â  Â  await new Promise(r => myVideoElement.onloadedmetadata = r);
Â  Â  Â  Â  Â  Â  myVideoElement.play();

Â  Â  Â  Â  Â  Â  // CRITICAL FIX: Set canvas size after video metadata is loaded
Â  Â  Â  Â  Â  Â  myCanvasElement.width = myVideoElement.videoWidth;
Â  Â  Â  Â  Â  Â  myCanvasElement.height = myVideoElement.videoHeight;
Â  Â  Â  Â  }

Â  Â  Â  Â  /*************************************************************
Â  Â  Â  Â  Â * ðŸš€ MOTION LISTENER (The sensor data collector)
Â  Â  Â  Â  Â *************************************************************/
Â  Â  Â  Â  function myStartMotionListener() {
Â  Â  Â  Â  Â  Â  myMotionData = [];
Â  Â  Â  Â  Â  Â  myLastSampleTime = Date.now();
Â  Â  Â  Â  Â  Â  myLastStatusUpdate = 0;

Â  Â  Â  Â  Â  Â  myMotionListener = (event) => {
Â  Â  Â  Â  Â  Â  Â  Â  if (!myIsMotionOrAnomalyModel) return;
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  const myNow = Date.now();
Â  Â  Â  Â  Â  Â  Â  Â  // Enforce sampling rate
Â  Â  Â  Â  Â  Â  Â  Â  if (myNow - myLastSampleTime < mySamplingIntervalMs) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return;Â 
Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  myLastSampleTime = myNow;

Â  Â  Â  Â  Â  Â  Â  Â  const acc = event.accelerationIncludingGravity;
Â  Â  Â  Â  Â  Â  Â  Â  if (!acc) return;

Â  Â  Â  Â  Â  Â  Â  Â  let x = acc.x || 0, y = acc.y || 0, z = acc.z || 0;
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  // Normalization (divide by g=9.81) is often required by ML models
Â  Â  Â  Â  Â  Â  Â  Â  const SCALE_FACTOR = 9.81;
Â  Â  Â  Â  Â  Â  Â  Â  x /= SCALE_FACTOR;
Â  Â  Â  Â  Â  Â  Â  Â  y /= SCALE_FACTOR;
Â  Â  Â  Â  Â  Â  Â  Â  z /= SCALE_FACTOR;

Â  Â  Â  Â  Â  Â  Â  Â  // Push normalized values (X, Y, Z)
Â  Â  Â  Â  Â  Â  Â  Â  myMotionData.push(x, y, z);

Â  Â  Â  Â  Â  Â  Â  Â  // FIXED: keep exactly seq_len*3 values
Â  Â  Â  Â  Â  Â  Â  Â  const maxLen = myMotionSequenceLength * 3;
Â  Â  Â  Â  Â  Â  Â  Â  if (myMotionData.length > maxLen) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myMotionData = myMotionData.slice(myMotionData.length - maxLen);
Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  // Status update only while NOT predicting (or periodically)
Â  Â  Â  Â  Â  Â  Â  Â  if (myNow - myLastStatusUpdate > 500) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myLastStatusUpdate = myNow;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const available = Math.floor(myMotionData.length / 3);
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myMotionOutputDiv.innerHTML =
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  `**Latest Accel:** X:${x.toFixed(2)} | Y:${y.toFixed(2)} | Z:${z.toFixed(2)}` +
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  `<br>Collecting Window: **${available}/${myMotionSequenceLength}** samples`;
Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  };

Â  Â  Â  Â  Â  Â  // ðŸ”‘ iOS permissions check
Â  Â  Â  Â  Â  Â  if (typeof DeviceMotionEvent.requestPermission === 'function') {
Â  Â  Â  Â  Â  Â  Â  Â  DeviceMotionEvent.requestPermission().then(state => {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if (state === 'granted') {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  window.addEventListener('devicemotion', myMotionListener);
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  } else {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myMotionOutputDiv.textContent = "Motion permission denied by user. Cannot use model.";
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  });
Â  Â  Â  Â  Â  Â  } else {
Â  Â  Â  Â  Â  Â  Â  Â  window.addEventListener('devicemotion', myMotionListener);
Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  myMotionOutputDiv.textContent = `Waiting for motion data (Window: ${myMotionSequenceLength} samples)â€¦`;
Â  Â  Â  Â  Â  Â  myClassificationOutputDiv.textContent = "Motion/Anomaly model active.";
Â  Â  Â  Â  }

Â  Â  Â  Â  function myStopMotionListener() {
Â  Â  Â  Â  Â  Â  if (myMotionListener)
Â  Â  Â  Â  Â  Â  Â  Â  window.removeEventListener('devicemotion', myMotionListener);
Â  Â  Â  Â  Â  Â  myMotionData = [];
Â  Â  Â  Â  Â  Â  myMotionOutputDiv.innerHTML = 'Motion listener stopped.';
Â  Â  Â  Â  }

Â  Â  Â  Â  /*************************************************************
Â  Â  Â  Â  Â * PRELOAD MODEL & READ METADATA
Â  Â  Â  Â  Â *************************************************************/
Â  Â  Â  Â  async function myPreLoadModelMetadata() {
Â  Â  Â  Â  Â  Â  const myUrl = myModelUrlInput.value.trim();
Â  Â  Â  Â  Â  Â  if (!myUrl) return;

Â  Â  Â  Â  Â  Â  // Stop all loops during load
Â  Â  Â  Â  Â  Â  if (myPredictionTimerId) clearInterval(myPredictionTimerId);
Â  Â  Â  Â  Â  Â  myPredictionTimerId = null;
Â  Â  Â  Â  Â  Â  myIsPredicting = false;
Â  Â  Â  Â  Â  Â  myVisualLoopRunning = false; 

Â  Â  Â  Â  Â  Â  myStopMotionListener();
Â  Â  Â  Â  Â  Â  myMotionOutputDiv.textContent = '';

Â  Â  Â  Â  Â  Â  // Read model type
Â  Â  Â  Â  Â  Â  const sel = myModelSelect.options[myModelSelect.selectedIndex];
Â  Â  Â  Â  Â  Â  const myModelType = sel.getAttribute('data-type');
Â  Â  Â  Â  Â  Â  const myLabels = JSON.parse(sel.getAttribute('data-labels') || '[]');
Â  Â  Â  Â  Â  Â  myCurrentModelConfig = { myType: myModelType, myLabels };

Â  Â  Â  Â  Â  Â  myIsMotionOrAnomalyModel = (myModelType === "motion" || myModelType === "anomaly");

Â  Â  Â  Â  Â  Â  await myInitializeLiteRT();

Â  Â  Â  Â  Â  Â  if (myModel) { try { myModel.delete(); } catch {} }
Â  Â  Â  Â  Â  Â  myModel = await LiteRT.loadAndCompile(myUrl, {accelerator:"webgpu"});

Â  Â  Â  Â  Â  Â  // INPUT DETAILS
Â  Â  Â  Â  Â  Â  myInputDetails = myModel.getInputDetails()[0];
Â  Â  Â  Â  Â  Â  const myInShape = myModel.getInputDetails()[0].shape;Â 
Â  Â  Â  Â  Â  Â  myInputDtype = myInputDetails.dtype || "float32";

Â  Â  Â  Â  Â  Â  // Fix motion input-shape detection
Â  Â  Â  Â  Â  Â  if (myIsMotionOrAnomalyModel) {
Â  Â  Â  Â  Â  Â  Â  Â  // Determine sequence length from the expected total flattened input N
Â  Â  Â  Â  Â  Â  Â  Â  const expectedTotalElements = myInShape.reduce((a, b) => a * b, 1);
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  // Motion model: [1, seq, 3] or [1, seq*3] or [1, 1, 1, seq*3] -> seq = N/3
Â  Â  Â  Â  Â  Â  Â  Â  if (myModelType === "motion") myMotionSequenceLength = expectedTotalElements / 3;Â 
Â  Â  Â  Â  Â  Â  Â  Â  // Anomaly model: [1, seq, 1] or [1, seq] or [1, 1, 1, seq] -> seq = N
Â  Â  Â  Â  Â  Â  Â  Â  else if (myModelType === "anomaly") myMotionSequenceLength = expectedTotalElements;Â 
Â  Â  Â  Â  Â  Â  Â  Â  else myMotionSequenceLength = 100; // Default fallback

Â  Â  Â  Â  Â  Â  Â  Â  myInputShapeSpan.textContent = `IMU sequence: ${myMotionSequenceLength} samples`;
Â  Â  Â  Â  Â  Â  } else {
Â  Â  Â  Â  Â  Â  Â  Â  const size = myInShape[1];
Â  Â  Â  Â  Â  Â  Â  Â  const channels = myInShape[3] || 1;
Â  Â  Â  Â  Â  Â  Â  Â  myInputShapeSpan.textContent = `${size}Ã—${size} (${channels} ch)`;
Â  Â  Â  Â  Â  Â  Â  Â  myInputDetails.myCurrentInputShape = size;
Â  Â  Â  Â  Â  Â  Â  Â  myInputDetails.myInputChannels = channels;
Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  myInputDtypeSpan.textContent = myInputDtype;

Â  Â  Â  Â  Â  Â  const myOut = myModel.getOutputDetails()[0];
Â  Â  Â  Â  Â  Â  myOutputDetailsSpan.textContent = myOut.shape.join("Ã—");

Â  Â  Â  Â  Â  Â  myClassificationOutputDiv.textContent = `Model Type: ${myModelType} loaded.`;
Â  Â  Â  Â  }

Â  Â  Â  Â  /*************************************************************
Â  Â  Â  Â  Â * LOAD MODEL & START PREDICTION (Hybrid approach)
Â  Â  Â  Â  Â *************************************************************/
Â  Â  Â  Â  async function myLoadModel() {
Â  Â  Â  Â  Â  Â  // Stop any existing loops
Â  Â  Â  Â  Â  Â  if (myPredictionTimerId) clearInterval(myPredictionTimerId);
Â  Â  Â  Â  Â  Â  myVisualLoopRunning = false;
Â  Â  Â  Â  Â  Â  myStopMotionListener();

Â  Â  Â  Â  Â  Â  // Ensure the webcam state is correct for the selected model type
Â  Â  Â  Â  Â  Â  await myEnableWebcam(myCameraSelect.value);

Â  Â  Â  Â  Â  Â  myIsPredicting = true;

Â  Â  Â  Â  Â  Â  if (myIsMotionOrAnomalyModel) {
Â  Â  Â  Â  Â  Â  Â  Â  // Start the time-based loop for motion models
Â  Â  Â  Â  Â  Â  Â  Â  myStartMotionListener();
Â  Â  Â  Â  Â  Â  Â  Â  // Renamed from myPredict to myMotionPredict
Â  Â  Â  Â  Â  Â  Â  Â  myPredictionTimerId = setInterval(myMotionPredict, myPredictionRateMs);Â 
Â  Â  Â  Â  Â  Â  } else {
Â  Â  Â  Â  Â  Â  Â  Â  // Start the frame-based loop for visual models
Â  Â  Â  Â  Â  Â  Â  Â  myVisualLoopRunning = true;
Â  Â  Â  Â  Â  Â  Â  Â  requestAnimationFrame(myVisualPredictLoop);
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  }

Â  Â  Â  Â  /*************************************************************
Â  Â  Â  Â  Â * myMotionPredict - Time-based loop for IMU/Sensor data
Â  Â  Â  Â  Â *************************************************************/
Â  Â  Â  Â  async function myMotionPredict() {
Â  Â  Â  Â  Â  Â  // Only runs if the current model is a motion/anomaly type
Â  Â  Â  Â  Â  Â  if (!myIsPredicting || !myModel || !myInputDetails || !myIsMotionOrAnomalyModel) return;

Â  Â  Â  Â  Â  Â  const isAnomalyModel = (myCurrentModelConfig.myType === "anomaly");
Â  Â  Â  Â  Â  Â  const seq = myMotionSequenceLength;
Â  Â  Â  Â  Â  Â  const available = Math.floor(myMotionData.length / 3);

Â  Â  Â  Â  Â  Â  // Only run inference if we have enough data for a full window
Â  Â  Â  Â  Â  Â  if ( (isAnomalyModel && available >= seq) || (!isAnomalyModel && available * 3 >= seq * 3) ) {

Â  Â  Â  Â  Â  Â  Â  Â  tf.tidy(() => {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  let inputBuffer = null;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  let expectedTotalElements = 0;Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if (isAnomalyModel) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // Anomaly model: requires magnitude calculation
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const magnitudes = [];
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for (let i = 0; i < seq; i++) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // Slice the last 'seq' samples from the full buffer
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const bufferIndex = myMotionData.length - (seq - i) * 3;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const x = myMotionData[bufferIndex] || 0;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const y = myMotionData[bufferIndex + 1] || 0;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const z = myMotionData[bufferIndex + 2] || 0;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  magnitudes.push(Math.sqrt(x*x + y*y + z*z));
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  inputBuffer = magnitudes;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  expectedTotalElements = seq;Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  } else {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // Motion Classification model: requires raw X,Y,Z sequence
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // Use the latest full sequence
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  inputBuffer = myMotionData.slice(myMotionData.length - seq * 3);
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  expectedTotalElements = seq * 3;Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // Create a 1D tensor and reshape it to the exact flattened format LiteRT expects: [1, 1, 1, N]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  let myT = tf.tensor(inputBuffer, [expectedTotalElements], myInputDtype);
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myT = myT.reshape([1, 1, 1, expectedTotalElements]);Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const myResults = LiteRTInterop.runWithTfjsTensors(myModel, myT);
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myDrawModelOutput(myResults);
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myResults.forEach(r => r.dispose());
Â  Â  Â  Â  Â  Â  Â  Â  });
Â  Â  Â  Â  Â  Â  } else {
Â  Â  Â  Â  Â  Â  Â  Â  // Update status if we are still collecting, even if inference didn't run
Â  Â  Â  Â  Â  Â  Â  Â  const available = Math.floor(myMotionData.length / 3);
Â  Â  Â  Â  Â  Â  Â  Â  myMotionOutputDiv.innerHTML =
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  `**Collecting:** **${available}/${myMotionSequenceLength}** samples`;
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  }


Â  Â  Â  Â  /*************************************************************
Â  Â  Â  Â  Â * myVisualPredictLoop - Frame-based loop for Camera data
Â  Â  Â  Â  Â *************************************************************/
Â  Â  Â  Â  function myVisualPredictLoop() {
Â  Â  Â  Â  Â  Â  // If running, but we switched to a motion model, stop this loop
Â  Â  Â  Â  Â  Â  if (!myVisualLoopRunning || myIsMotionOrAnomalyModel || !myModel) {
Â  Â  Â  Â  Â  Â  Â  Â  return;
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  tf.tidy(() => {
Â  Â  Â  Â  Â  Â  Â  Â  let myT = tf.browser.fromPixels(myVideoElement);

Â  Â  Â  Â  Â  Â  Â  Â  const size = myInputDetails.myCurrentInputShape;
Â  Â  Â  Â  Â  Â  Â  Â  const channels = myInputDetails.myInputChannels;

Â  Â  Â  Â  Â  Â  Â  Â  if (channels === 1) myT = myT.mean(2, true);
Â  Â  Â  Â  Â  Â  Â  Â  myT = myT.resizeBilinear([size, size]);

Â  Â  Â  Â  Â  Â  Â  Â  if (myInputDtype.includes("float")) myT = myT.div(255);
Â  Â  Â  Â  Â  Â  Â  Â  else myT = myT.cast(myInputDtype);

Â  Â  Â  Â  Â  Â  Â  Â  myT = myT.expandDims();

Â  Â  Â  Â  Â  Â  Â  Â  const myResults = LiteRTInterop.runWithTfjsTensors(myModel, myT);
Â  Â  Â  Â  Â  Â  Â  Â  myDrawModelOutput(myResults);
Â  Â  Â  Â  Â  Â  Â  Â  myResults.forEach(r => r.dispose());
Â  Â  Â  Â  Â  Â  });
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  // Schedule the next frame to keep the loop going
Â  Â  Â  Â  Â  Â  requestAnimationFrame(myVisualPredictLoop);
Â  Â  Â  Â  }

Â  Â  Â  Â  /*************************************************************
Â  Â  Â  Â  Â * DRAW OUTPUT (Handles all types)
Â  Â  Â  Â  Â *************************************************************/
Â  Â  Â  Â  async function myDrawModelOutput(myResults) {
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  // Clear canvas only if it's a visual model
Â  Â  Â  Â  Â  Â  if (!myIsMotionOrAnomalyModel) {
Â  Â  Â  Â  Â  Â  Â  Â  myCanvasContext.clearRect(0, 0, myCanvasElement.width, myCanvasElement.height);
Â  Â  Â  Â  Â  Â  Â  Â  const w = myCanvasElement.width;
Â  Â  Â  Â  Â  Â  Â  Â  const h = myCanvasElement.height;
Â  Â  Â  Â  Â  Â  Â  Â  myCanvasContext.fillStyle = "rgba(0,0,0,0.4)";
Â  Â  Â  Â  Â  Â  Â  Â  myCanvasContext.fillRect(0, h-40, w, 40);
Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  const myType = myCurrentModelConfig.myType;
Â  Â  Â  Â  Â  Â  const myLabels = myCurrentModelConfig.myLabels;
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  // Clear visual output when running sensor models
Â  Â  Â  Â  Â  Â  if (myIsMotionOrAnomalyModel) {
Â  Â  Â  Â  Â  Â  Â  Â  myClassificationOutputDiv.textContent = "";
Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  switch (myType) {

Â  Â  Â  Â  Â  Â  Â  Â  /******** MOTION CLASSIFICATION ********/
Â  Â  Â  Â  Â  Â  Â  Â  case "motion": {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const probs = myResults[0].dataSync();
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const top = [...probs].map((v,i)=>({v,i}))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .sort((a,b)=>b.v-a.v).slice(0,3);

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  let html = "<strong>Motion Classification:</strong><br>";
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  top.forEach(t => {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  html += `${myLabels[t.i] || ("Class "+t.i)}:Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <strong>${(t.v*100).toFixed(1)}%</strong><br>`;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  });
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myMotionOutputDiv.innerHTML = html; // Output to the designated motion div
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break;
Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  /******** ANOMALY ********/
Â  Â  Â  Â  Â  Â  Â  Â  case "anomaly": {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // Anomaly models often output a single score, usually the last value
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const arr = myResults[0].dataSync();
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const score = arr[arr.length - 1] * 100;

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  let html = `<strong>Anomaly Score:</strong>
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <strong>${score.toFixed(1)}%</strong><br>`;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  html += score > 50
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ? "<span style='color:red; font-weight:bold;'>âš  ANOMALY DETECTED!</span>"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  : "<span style='color:green; font-weight:bold;'>âœ“ Normal Movement</span>";
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myMotionOutputDiv.innerHTML = html; // Output to the designated motion div
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break;
Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  /******** CLASSIFICATION ********/
Â  Â  Â  Â  Â  Â  Â  Â  case "classification": {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const arr = myResults[0].dataSync();
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const top = [...arr].map((v,i)=>({v,i}))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .sort((a,b)=>b.v-a.v).slice(0,3);
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  let html = "Classification:<br>";
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  top.forEach(t=>{
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  html += `${myLabels[t.i] || ("Class "+t.i)}:Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <strong>${(t.v*100).toFixed(1)}%</strong><br>`;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  });
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myClassificationOutputDiv.innerHTML = html;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break;
Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  /******** OBJECT DETECTION ********/
Â  Â  Â  Â  Â  Â  Â  Â  case "object_detection": {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const out = myResults[0].dataSync();
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const dim = myResults[0].shape[1];
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const numCls = myResults[0].shape[3] - 1;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const thresh = 0.5;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  let det = [];

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for (let i=0; i<out.length; i += numCls+1) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const idx = Math.floor(i / (numCls+1));
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const cx = idx % dim;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const cy = Math.floor(idx / dim);

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for (let c=0; c<numCls; c++) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const score = out[i+c+1];
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if (score > thresh) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  det.push({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  score, label: myLabels[c] || "cls"+c, x:cx, y:cy
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  });
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  let html = `Object Detection (>${thresh*100}%):<br>`;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const w = myCanvasElement.width;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const h = myCanvasElement.height;

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myCanvasContext.font = '14px sans-serif';Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myCanvasContext.textAlign = 'start';
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myCanvasContext.textBaseline = 'bottom';

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  det.forEach(d=>{
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  html += `${d.label}: ${(d.score*100).toFixed(1)}%<br>`;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const X = (d.x/dim)*w;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const Y = (d.y/dim)*h;

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myCanvasContext.strokeStyle = "red";
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myCanvasContext.lineWidth = 3;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myCanvasContext.strokeRect(X-15, Y-15, 30,30);
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myCanvasContext.fillStyle="red";
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myCanvasContext.fillText(d.label, X-15, Y-20);
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  });

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myClassificationOutputDiv.innerHTML = html;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break;
Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  /******** DEPTH ********/
Â  Â  Â  Â  Â  Â  Â  Â  case "depth": {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const depthTensor = myResults[0].squeeze();
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tf.tidy(() => {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const min = depthTensor.min();
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const max = depthTensor.max();
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const norm = depthTensor.sub(min).div(max.sub(min)).mul(255).toInt();
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const arr = norm.dataSync();

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const h = depthTensor.shape[0];
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const w = depthTensor.shape[1];
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const img = myCanvasContext.createImageData(w,h);

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for (let i=0;i<arr.length;i++){
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  img.data[i*4]=arr[i];
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  img.data[i*4+1]=arr[i];
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  img.data[i*4+2]=arr[i];
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  img.data[i*4+3]=255;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const tmp = document.createElement("canvas");
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tmp.width=w; tmp.height=h;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tmp.getContext("2d").putImageData(img,0,0);
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myCanvasContext.drawImage(tmp,0,0,myCanvasElement.width,myCanvasElement.height);
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  });
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break;
Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  }

Â  Â  Â  Â  /*************************************************************
Â  Â  Â  Â  Â * UI HANDLERS (using inline onclick preferences)
Â  Â  Â  Â  Â *************************************************************/
Â  Â  Â  Â  myLoadButton.onclick = myLoadModel;
Â  Â  Â  Â  myCameraSelect.onchange = ()=>myEnableWebcam(myCameraSelect.value);
Â  Â  Â  Â  myModelSelect.onchange = ()=>{
Â  Â  Â  Â  Â  Â  myModelUrlInput.value = myModelSelect.value;
Â  Â  Â  Â  Â  Â  myPreLoadModelMetadata();
Â  Â  Â  Â  };
Â  Â  Â  Â  myModelUrlInput.onchange = myPreLoadModelMetadata;

Â  Â  Â  Â  /*************************************************************
Â  Â  Â  Â  Â * POPULATE UI & START
Â  Â  Â  Â  Â *************************************************************/
Â  Â  Â  Â  function myPopulate() {
Â  Â  Â  Â  Â  Â  myDepthModels.forEach((m, i)=>{
Â  Â  Â  Â  Â  Â  Â  Â  const o=document.createElement("option");
Â  Â  Â  Â  Â  Â  Â  Â  o.value=m.myUrl;
Â  Â  Â  Â  Â  Â  Â  Â  o.text=m.myName;
Â  Â  Â  Â  Â  Â  Â  Â  o.setAttribute("data-type", m.myType);
Â  Â  Â  Â  Â  Â  Â  Â  o.setAttribute("data-labels", JSON.stringify(m.myLabels || []));
Â  Â  Â  Â  Â  Â  Â  Â  myModelSelect.appendChild(o);
Â  Â  Â  Â  Â  Â  Â  Â  // Pre-select the Motion Classification model for easy testing
Â  Â  Â  Â  Â  Â  Â  Â  if (m.myName.includes("Motion Classification")) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myModelSelect.selectedIndex = i;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  myModelUrlInput.value = m.myUrl;
Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  });
Â  Â  Â  Â  Â  Â  myPreLoadModelMetadata();
Â  Â  Â  Â  }

Â  Â  Â  Â  myInitializeLiteRT();
Â  Â  Â  Â  myGetCameras();
Â  Â  Â  Â  myEnableWebcam();
Â  Â  Â  Â  myPopulate();

Â  Â  Â  Â  async function myGetCameras() {
Â  Â  Â  Â  Â  Â  const dev = await navigator.mediaDevices.enumerateDevices();
Â  Â  Â  Â  Â  Â  const cams = dev.filter(d=>d.kind==="videoinput");
Â  Â  Â  Â  Â  Â  myCameraSelect.innerHTML="";
Â  Â  Â  Â  Â  Â  cams.forEach((d,i)=>{
Â  Â  Â  Â  Â  Â  Â  Â  const o=document.createElement("option");
Â  Â  Â  Â  Â  Â  Â  Â  o.value=d.deviceId;
Â  Â  Â  Â  Â  Â  Â  Â  o.text=`Camera ${d.label || ("Camera "+(i+1))}`;Â 
Â  Â  Â  Â  Â  Â  Â  Â  myCameraSelect.appendChild(o);
Â  Â  Â  Â  Â  Â  });
Â  Â  Â  Â  }
Â  Â  </script>
</head>

<body>

Â  Â  <h1 style="color:#fff; font-size:1.5em; text-align:center;">LiteRT.js Multi-Model Visualizer</h1>
Â  Â Â 
Â  Â  <div id="videoContainer">
Â  Â  Â  Â  <video id="webcam" autoplay playsinline></video>
Â  Â  Â  Â  <canvas id="myCanvas"></canvas>
Â  Â  </div>

Â  Â  <div id="controlsPanel">
Â  Â  Â  Â  <h3>Model Selection & Controls</h3>
Â  Â  Â  Â  <div><label for="cameraSelect">Camera:</label> <select id="cameraSelect"></select></div>
Â  Â  Â  Â  <div><label for="modelSelect">Model Preset:</label> <select id="modelSelect"></select></div>

Â  Â  Â  Â  <input id="myModelUrlInput" type="url" placeholder="or enter model URL" />

Â  Â  Â  Â  <div style="margin-top: 5px; text-align: center;">
Â  Â  Â  Â  Â  Â  <button id="loadModelButton" style="padding: 10px 20px;">**Load & Start Prediction**</button>
Â  Â  Â  Â  </div>

Â  Â  Â  Â  <hr>

Â  Â  Â  Â  <h4>Model Details</h4>
Â  Â  Â  Â  <div style="font-size:12px;">
Â  Â  Â  Â  Â  Â  Input Shape: <span id="myInputShapeStatus">N/A</span><br>
Â  Â  Â  Â  Â  Â  Input DType: <span id="myInputDtypeStatus">N/A</span><br>
Â  Â  Â  Â  Â  Â  Output Shape: <span id="myOutputDetailsStatus">N/A</span>
Â  Â  Â  Â  </div>

Â  Â  Â  Â  <hr>
Â  Â  Â  Â Â 
Â  Â  Â  Â  <h4>Prediction Results</h4>
Â  Â  Â  Â Â 
Â  Â  Â  Â  <div id="myMotionOutput"
Â  Â  Â  Â  Â  Â  Â style="margin-top:10px; padding:5px; border:1px solid #c9f; min-height:30px; background:#f0f0ff;">
Â  Â  Â  Â  Â  Â  Motion/Anomaly Status
Â  Â  Â  Â  </div>
Â  Â  Â  Â Â 
Â  Â  Â  Â  <div id="myClassificationOutput"
Â  Â  Â  Â  Â  Â  Â style="margin-top:10px; padding:5px; border:1px solid #ccc; min-height:30px; color: #000; background: #fff;">
Â  Â  Â  Â  Â  Â  Model Type: N/A
Â  Â  Â  Â  </div>
Â  Â  </div>

</body>
</html>
