<!--
Webcam FOMO Object Detection using TensorFlow.js and a locally loaded TFLite model.
This application captures live video, preprocesses frames, runs inference using the
Edge Impulse FOMO model format, and displays results on the canvas.
-->
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>myFOMO Live Detection</title>
    <!-- Minimal inline CSS for a simple, clean layout -->
    <style>
        body {
            font-family: Arial, sans-serif;
            display: flex;
            flex-direction: column;
            align-items: center;
            margin: 20px;
            background-color: #f4f4f9;
        }
        #myControls {
            margin-bottom: 20px;
            padding: 15px;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }
        #myVideo, #myCanvas {
            border: 2px solid #ccc;
            border-radius: 8px;
            margin-top: 10px;
            background-color: #333;
        }
        /* Hide the video element since we draw to canvas */
        #myVideo { display: none; }

        /* Ensure the canvas is visible and maintains a consistent size */
        #myCanvas {
            width: 240px;
            height: 240px;
        }
        .myButton {
            background-color: #1d4ed8; /* Blue button for action */
            color: white;
            padding: 10px 20px;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-size: 16px;
            transition: background-color 0.3s;
        }
        .myButton:hover {
            background-color: #1e40af;
        }
        .myStatus {
            margin-top: 10px;
            font-weight: bold;
            color: #333;
            text-align: center;
        }
    </style>

    <!-- Load TensorFlow.js (for utilities like tf.fromPixels, tf.dispose) -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.2.0/dist/tf.min.js"></script>
    <!-- Load TensorFlow Lite for JS (to interpret the .tflite model) -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-tflite@0.0.1-alpha.9/dist/tf-tflite.min.js"></script>
</head>
<body>

    <div id="myControls">
        <button class="myButton" id="myStartButton" onclick="myStartDetection()">
            1. Load fomo01.tflite and Start Webcam
        </button>
        <p class="myStatus" id="myStatusMessage">Click the button to begin...</p>
    </div>

    <!-- Video element to stream the webcam -->
    <video id="myVideo" playsinline autoplay></video>

    <!-- Canvas to display the processed video stream (240x240 input size) -->
    <canvas id="myCanvas" width="240" height="240"></canvas>

    <script>
        // Use my prefix for global variables as per preference
        let myTfLiteModel = null;
        let myIsWebcamReady = false;
        let myCanvasElement = null;
        let myVideoElement = null;
        let myContext = null;
        let myStatusElement = null;
        let myAnimationFrameId = null;

        // Configuration for the FOMO model
        const myInputSize = 240;
        const myGridFactor = 8; // FOMO default spatial reduction (240/8 = 30)
        const myGridSize = myInputSize / myGridFactor; // 30x30 grid
        const myDetectionThreshold = 0.5; // Minimum probability for detection
        const myClassLabels = ['background', 'object', 'another_object', 'class_N']; // Placeholder labels

        window.onload = function() {
            myCanvasElement = document.getElementById('myCanvas');
            myVideoElement = document.getElementById('myVideo');
            myStatusElement = document.getElementById('myStatusMessage');
            myContext = myCanvasElement.getContext('2d');
        }
        
        /**
         * @function myStartDetection
         * Loads the model directly from the local file system using its filename (URL).
         * This replaces the file input/upload handler.
         */
        async function myStartDetection() {
            // Disable the button to prevent multiple loads
            document.getElementById('myStartButton').disabled = true;
            myStatusElement.textContent = 'Loading model fomo01.tflite... Please wait.';
            
            // Clear previous detection loop if any
            myStopInferenceLoop();

            try {
                // --- CRITICAL CHANGE: Load model directly from URL ---
                // Assumes 'fomo01.tflite' is in the same directory as this HTML file.
                myTfLiteModel = await tflite.loadTFLiteModel('fomo01.tflite');
                
                myStatusElement.textContent = 'Model loaded successfully! Starting webcam...';
                console.log('TFLite Model Loaded.');

                // Now start the webcam
                await mySetupWebcam();

            } catch (myError) {
                myStatusElement.textContent = `Failed to load model from fomo01.tflite: ${myError.message}. Ensure the file is present in the same directory.`;
                console.error('Model loading error:', myError);
                document.getElementById('myStartButton').disabled = false;
            }
        }


        /**
         * @function mySetupWebcam
         * Initializes the webcam stream and prepares the video element.
         */
        async function mySetupWebcam() {
            if (myIsWebcamReady) return;

            try {
                const myStream = await navigator.mediaDevices.getUserMedia({
                    video: {
                        width: myInputSize,
                        height: myInputSize,
                        facingMode: 'user'
                    },
                    audio: false
                });

                myVideoElement.srcObject = myStream;

                await new Promise((myResolve) => {
                    myVideoElement.onloadedmetadata = () => {
                        myResolve();
                    };
                });

                myVideoElement.play();
                myIsWebcamReady = true;
                myStatusElement.textContent = 'Webcam active. Running inference... (Check Console)';

                // Start the continuous inference loop
                myRunInferenceLoop();

            } catch (myError) {
                myStatusElement.textContent = `Error accessing webcam: ${myError.message}. Make sure you grant camera access.`;
                console.error('Webcam access error:', myError);
                document.getElementById('myStartButton').disabled = false;
            }
        }

        /**
         * @function myRunInferenceLoop
         * The main loop for frame capture and model inference.
         */
        function myRunInferenceLoop() {
            // Cancel previous loop if running
            if (myAnimationFrameId) {
                cancelAnimationFrame(myAnimationFrameId);
            }

            // TensorFlow.js utility to manage tensor memory automatically
            tf.tidy(() => {
                if (!myTfLiteModel || !myIsWebcamReady) {
                    return;
                }

                // 1. Capture and Preprocess Frame
                myContext.drawImage(myVideoElement, 0, 0, myInputSize, myInputSize);

                // Convert canvas image data to a TensorFlow tensor
                let myInputTensor = tf.browser.fromPixels(myCanvasElement);

                // Resize and Normalize (0-1)
                let myNormalizedTensor = myInputTensor
                    .resizeBilinear([myInputSize, myInputSize])
                    .cast('float32')
                    .div(255.0)
                    .expandDims(0); // Add batch dimension

                // 2. Run Inference
                const myOutputTensor = myTfLiteModel.predict(myNormalizedTensor);

                // 3. Post-processing (FOMO-specific)
                const myPredictionData = myOutputTensor.dataSync();
                const myOutputShape = myOutputTensor.shape;
                const myNumClasses = myOutputShape[3]; // C = number of classes (including background)
                const myGridX = myOutputShape[1];
                const myGridY = myOutputShape[2];

                if (myGridX !== myGridSize || myGridY !== myGridSize) {
                    console.error(`ERROR: Model output shape mismatch. Expected grid size ${myGridSize}x${myGridSize}, but got ${myGridX}x${myGridY}. Skipping frame.`);
                    return;
                }

                let myDetectionsFound = false;

                // Iterate over the grid (i=row, j=column)
                for (let i = 0; i < myGridSize; i++) {
                    for (let j = 0; j < myGridSize; j++) {
                        // Find the index of the class data for the current cell (i, j)
                        const myCellStartIndex = (i * myGridSize * myNumClasses) + (j * myNumClasses);

                        let myMaxProbability = 0;
                        let myMaxClassIndex = 0;

                        // Find the highest probability class
                        for (let k = 0; k < myNumClasses; k++) {
                            const myProbability = myPredictionData[myCellStartIndex + k];
                            if (myProbability > myMaxProbability) {
                                myMaxProbability = myProbability;
                                myMaxClassIndex = k;
                            }
                        }

                        // Check if probability is above threshold and it's NOT the background class (index 0)
                        if (myMaxClassIndex !== 0 && myMaxProbability >= myDetectionThreshold) {
                            myDetectionsFound = true;

                            // Calculate the centroid coordinates in the 240x240 input space
                            const myCentroidX = (j * myGridFactor) + (myGridFactor / 2);
                            const myCentroidY = (i * myGridFactor) + (myGridFactor / 2);

                            // Log the result to the console
                            console.log(`[Detection] Class: ${myClassLabels[myMaxClassIndex]}, Conf: ${myMaxProbability.toFixed(3)}, Centroid: (${myCentroidX}, ${myCentroidY})`);

                            // OPTIONAL: Draw a visualization on the canvas
                            myContext.fillStyle = myMaxClassIndex === 1 ? 'rgba(255, 0, 0, 0.7)' : 'rgba(0, 255, 0, 0.7)';
                            myContext.fillRect(myCentroidX - 4, myCentroidY - 4, 8, 8); // Draw a small square at the centroid
                            myContext.font = '10px Arial';
                            myContext.fillStyle = 'white';
                            myContext.textAlign = 'center';
                            myContext.fillText(myClassLabels[myMaxClassIndex], myCentroidX, myCentroidY - 6);
                        }
                    }
                }

                if (!myDetectionsFound) {
                    // console.log('No object detected above threshold:', myDetectionThreshold.toFixed(2));
                }
            }); // tf.tidy cleans up tensors automatically

            // Request the next frame
            myAnimationFrameId = requestAnimationFrame(myRunInferenceLoop);
        }

        /**
         * @function myStopInferenceLoop
         * Stops the continuous frame request loop.
         */
        function myStopInferenceLoop() {
            if (myAnimationFrameId) {
                cancelAnimationFrame(myAnimationFrameId);
                myAnimationFrameId = null;
            }
        }
    </script>
</body>
</html>

