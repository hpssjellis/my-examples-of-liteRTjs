<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0" />
    <title>LiteRT.js MINST Classification Demo</title>
    
    <script type="module">
        // --- Imports (Using correct ES Modules) ---
        import * as LiteRT from 'https://cdn.jsdelivr.net/npm/@litertjs/core@0.2.1/+esm';
        import * as LiteRTInterop from 'https://cdn.jsdelivr.net/npm/@litertjs/tfjs-interop/+esm';
        
        import * as tf from 'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/+esm'; 
        import 'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgpu/+esm'; 
        
        // --- Configuration (Will be updated after loading model) ---
        const myDefaultConfig = {
            // Using a model that seems to have been trained on a large image pipeline
            myDefaultUrl: 'https://hpssjellis.github.io/my-examples-of-liteRTjs/public/tflite/MINST.tflite',
            myConfidenceThreshold: 0.5,
            myClassLabels: ['background', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9'],
            // These will be overridden by the model's actual requirements (likely 320, 3)
            myInputResolution: 28, 
            myInputChannels: 1, 
            myModelType: 'classification', 
        };

        // --- Global Variables (my-prefixed) ---
        const myModelUrl = myDefaultConfig.myDefaultUrl;
        let myModel = undefined;
        let myIsPredicting = false;
        let myTimerIntervalId = null; 
        let myInitialPredictionStartTime = 0;

        // --- DOM Elements ---
        const myVideoElement = document.getElementById('webcam');
        const myOverlayCanvas = document.getElementById('overlay');
        const myLoadButton = document.getElementById('loadModelButton');
        const myCameraSelect = document.getElementById('cameraSelect');
        const myLoadStatusSpan = document.getElementById('myLoadStatus');
        const myAnalysisStatusSpan = document.getElementById('myAnalysisStatus');
        const myResultSpan = document.getElementById('myResult'); 

        // --- Timer Functions ---

        /** Starts a count-up timer. */
        function myStartLoadTimer() {
            let myStartTime = performance.now();
            myLoadStatusSpan.textContent = '0.0s';
            myTimerIntervalId = setInterval(() => {
                let myElapsedTime = performance.now() - myStartTime;
                myLoadStatusSpan.textContent = `${(myElapsedTime / 1000).toFixed(1)}s`;
            }, 100);
        }

        /** Stops and clears the count-up timer. */
        function myStopLoadTimer(myFinalMessage) {
            if (myTimerIntervalId !== null) {
                clearInterval(myTimerIntervalId);
                myTimerIntervalId = null;
            }
            myLoadStatusSpan.textContent = myFinalMessage;
        }

        // --- Core Functions ---
        
        /** Populates the camera selection dropdown. */
        async function myGetCameras() {
            if (!navigator.mediaDevices || !navigator.mediaDevices.enumerateDevices) {
                console.warn('enumerateDevices() not supported.');
                return;
            }

            try {
                await navigator.mediaDevices.getUserMedia({ video: true, audio: false });
                const myDevices = await navigator.mediaDevices.enumerateDevices();
                const myVideoDevices = myDevices.filter(myDevice => myDevice.kind === 'videoinput');
                
                myCameraSelect.innerHTML = '';
                
                if (myVideoDevices.length === 0) {
                    myCameraSelect.innerHTML = '<option>No Camera Found</option>';
                    myCameraSelect.disabled = true;
                    return;
                }

                myVideoDevices.forEach((myDevice, myIndex) => {
                    const myOption = document.createElement('option');
                    myOption.value = myDevice.deviceId;
                    myOption.text = myDevice.label || `Camera ${myIndex + 1}`; 
                    myCameraSelect.appendChild(myOption);
                });

                myCameraSelect.disabled = false;
                
            } catch (e) {
                console.error('Error listing devices:', e);
            }
        }
        
        /** Enables the webcam. */
        async function myEnableWebcam(myDeviceId) {
            if (myVideoElement.srcObject) {
                myVideoElement.srcObject.getTracks().forEach(track => track.stop());
            }

            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                console.error('Browser API navigator.mediaDevices.getUserMedia not available');
                return;
            }
            
            const myVideoConfig = {
                'audio': false,
                'video': {
                    deviceId: myDeviceId ? { exact: myDeviceId } : undefined,
                    width: 640,
                    height: 480
                }
            };
            
            try {
                const myStream = await navigator.mediaDevices.getUserMedia(myVideoConfig);
                myVideoElement.srcObject = myStream;
                await new Promise(resolve => myVideoElement.onloadedmetadata = resolve);
                console.log('Video stream started successfully.');
            } catch (e) {
                console.error('Error enabling webcam:', e);
            }
        }

        /** Loads the LiteRT environment, initializes TF.js, and loads the Model. */
        async function myLoadModel() {
            if (myModel !== undefined || myIsPredicting) return;
            
            myLoadButton.textContent = 'Loading...';
            myLoadButton.disabled = true;
            myCameraSelect.disabled = true;
            myAnalysisStatusSpan.textContent = 'Waiting for Model...';

            try {
                // 1. Start Webcam
                await myEnableWebcam(myCameraSelect.value);

                // 2. Start the LOAD timer
                myStartLoadTimer();
                
                // 3. AWAIT TF.JS INITIALIZATION (Required for WebGPU to fully initialize)
                console.log('Awaiting TF.js backend readiness...');
                await tf.ready(); 

                // 4. Load LiteRT Wasm 
                await LiteRT.loadLiteRt('https://cdn.jsdelivr.net/npm/@litertjs/core@0.2.1/wasm/');
                
                // 5. Connect LiteRT to the initialized TF.js backend
                const myTfBackend = tf.backend(); 
                if (myTfBackend.device) {
                    LiteRT.setWebGpuDevice(myTfBackend.device);
                    console.log(`LiteRT connected to TF.js backend: ${myTfBackend.name}`);
                } else {
                    console.warn(`Could not set WebGPU device. Using fallback backend: ${myTfBackend.name}`);
                }
                
                console.log('Loading and compiling model...');
                myModel = await LiteRT.loadAndCompile(myModelUrl, {
                    accelerator: 'webgpu', 
                });
                
                // ** CRITICAL FIX: Dynamically get model input shape and update config **
                const myInputDetails = myModel.getInputDetails()[0];
                const myRequiredShape = myInputDetails.shape;
                
                myDefaultConfig.myInputResolution = myRequiredShape[1]; 
                myDefaultConfig.myInputChannels = myRequiredShape[3];
                console.log(`Model input shape dynamically set to: [${myRequiredShape[1]}x${myRequiredShape[2]}x${myRequiredShape[3]}]`);
                
                // 6. Stop the LOAD timer and update status
                myStopLoadTimer(`Loaded: ${myLoadStatusSpan.textContent}`);
                
                // 7. Start Prediction Loop
                console.log('Model loaded. Starting prediction timer.');
                myLoadButton.textContent = 'Compiling...';
                myAnalysisStatusSpan.textContent = 'Compiling/First Run...';
                myInitialPredictionStartTime = performance.now();

                myIsPredicting = true;
                myPredict();
                
            } catch (e) {
                myStopLoadTimer('Error');
                console.error('Error during model loading or setup:', e);
                myLoadButton.textContent = 'Error Loading Model';
            }
        }
        
        /**
         * Main prediction loop.
         */
        async function myPredict() {
            if (myModel === undefined || !myIsPredicting) return;
            
            // --- First Run Compilation Timer Logic ---
            if (myInitialPredictionStartTime > 0) {
                await tf.backend().queue.onSubmittedWorkDone();
                const myInitialDelay = performance.now() - myInitialPredictionStartTime;
                myAnalysisStatusSpan.textContent = `Delay: ${(myInitialDelay / 1000).toFixed(2)}s`;
                myLoadButton.textContent = 'Running';
                myInitialPredictionStartTime = 0;
            }

            // The core inference block
            tf.tidy(() => {
                // 1. Pre-processing: Capture, Resize, and Channel Conversion
                const myResizeDimension = myDefaultConfig.myInputResolution; // e.g., 320
                const myRequiredChannels = myDefaultConfig.myInputChannels;   // e.g., 3
                
                let myInputTensor = tf.browser.fromPixels(myVideoElement);

                // Resize to model input size
                myInputTensor = myInputTensor.resizeBilinear([myResizeDimension, myResizeDimension]);

                // *** FIX: Handle Channel Mismatch (Grayscale 1-ch to RGB 3-ch) ***
                if (myInputTensor.shape[2] !== myRequiredChannels) {
                    // Convert to Grayscale (1-channel)
                    let myGrayscaleTensor = myInputTensor.mean(2).expandDims(2); 
                    // Tile 1-channel into 3 identical channels (e.g., 320x320x3)
                    myInputTensor.dispose(); // Dispose of the original tensor
                    myInputTensor = myGrayscaleTensor.tile([1, 1, myRequiredChannels]);
                }
                
                // Final normalization and batch dimension
                myInputTensor = myInputTensor
                    .div(255.0) // Normalize to 0-1
                    .expandDims(0); // Add batch dimension (1x320x320x3)
                
                // 2. Inference
                const myResults = LiteRTInterop.runWithTfjsTensors(myModel, myInputTensor);
                
                const myOutputTensor = myResults[0]; 
                
                // 3. Post-processing: Softmax, Get Prediction, and Display
                const myProbabilities = myOutputTensor.softmax(); 
                
                const myPredictionIndex = myProbabilities.argMax(1).dataSync()[0]; 
                const myConfidence = myProbabilities.max().dataSync()[0];

                for (const output of myResults) {
                    output.dispose();
                }
                
                // 4. Display Results
                if (myConfidence >= myDefaultConfig.myConfidenceThreshold) {
                    const myLabel = myDefaultConfig.myClassLabels[myPredictionIndex];
                    myResultSpan.textContent = `${myLabel} (${(myConfidence * 100).toFixed(1)}%)`;
                } else {
                    myResultSpan.textContent = `Thinking... (${(myConfidence * 100).toFixed(1)}%)`;
                }
                
                // Draw the resized input for visualization on the overlay canvas
                // We use the 3-channel tensor to draw, tile it back to 3 channels if it somehow lost them
                tf.browser.toPixels(myInputTensor.squeeze(), myOverlayCanvas); 

            });
            
            if (myInitialPredictionStartTime === 0) {
                await tf.backend().queue.onSubmittedWorkDone();
            }
            
            requestAnimationFrame(myPredict);
        }

        /** Handles camera changes. */
        async function myCameraChangeHandler() {
            await myEnableWebcam(myCameraSelect.value);
        }

        // --- Initialization and Static Links ---
        
        myLoadButton.onclick = myLoadModel;
        myCameraSelect.onchange = myCameraChangeHandler;

        myGetCameras();
        myEnableWebcam();
    </script>
</head>
<body style="margin:0; padding:0; height:100vh; display:flex; flex-direction:column; align-items:center; justify-content:center; background-color:#111; color:white;">
    <div id="controls" style="position:relative; z-index:2; margin-bottom:10px; padding:10px; background:rgba(0, 0, 0, 0.7); border-radius:5px;">
        <label for="cameraSelect" style="margin-right:10px;">Camera:</label>
        <select id="cameraSelect" style="padding:5px;">
            <option value="">Loading Cameras...</option>
        </select>
        <button id="loadModelButton" style="padding:5px 10px; background-color:#4CAF50; color:white; border:none; border-radius:3px;">Load Model & Start</button>
        <br>
        <span style="font-size:12px; margin-right:10px;">Load Time: <span id="myLoadStatus" style="font-weight:bold;">Ready</span></span>
        <span style="font-size:12px;">Analysis Delay: <span id="myAnalysisStatus" style="font-weight:bold;">N/A</span></span>
        <h3 style="margin:5px 0 0 0; text-align:center;">
            Prediction: <span id="myResult" style="color:#FFA726; font-size:1.2em;">---</span>
        </h3>
    </div>

    <div style="position:relative; width:640px; height:480px; border: 2px solid #555;">
        <video id="webcam" autoplay playsinline style="position:absolute; width:100%; height:100%; object-fit:cover; z-index:0;"></video>
        
        <canvas id="overlay" width="640" height="480" 
            style="position:absolute; top:0; left:0; width:100%; height:100%; 
                   pointer-events:none; z-index:1; opacity:0.8; 
                   image-rendering: pixelated;"></canvas>
    </div>
</body>
</html>
